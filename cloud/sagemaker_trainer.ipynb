{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS Account Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "# User input\n",
    "role_name = \"Developer\" \n",
    "\n",
    "# Sagemaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "try:\n",
    "    # Run notebook from Sagemaker studio\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    # Run notebook from local machine\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName=role_name)['Role']['Arn']\n",
    "    print(\"Get role successfully\")\n",
    "account = sagemaker_session.boto_session.client('sts').get_caller_identity()['Account']\n",
    "region = sagemaker_session.boto_session.region_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build & Push Docker Image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description**\n",
    "- This section deals with the variables related to Docker images that will be pushed to the Elastic Container Registry (ECR) after the docker image is built.\n",
    "- Usually, there's no need to build the Docker image more than once because all source codes will be packed and sent to S3 storage. Thefore, `is_build` argument must set to `False` unless the docker image is not available on ECR or `sagemaker_main.py` and `sagemaker_utils.py` are modified \n",
    "- Any changes made to the source code will not affect the Docker image.\n",
    "\n",
    "**Before Running**\n",
    "- Please change `bucket_name` and don't forget to have docker desktop running"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables for Docker Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker_utils import create_bucket_if_not_exists\n",
    "image = 'cog_verse'\n",
    "base_job_name = 'cog-verse-training'\n",
    "bucket_name = \"cog-verse\"\n",
    "is_build = \"false\" # whether to build docker\n",
    "instance_type = 'ml.m5.4xlarge'\n",
    "\n",
    "# Create an S3 client\n",
    "create_bucket_if_not_exists(bucket_name=bucket_name, region=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"image\"] = str(image)\n",
    "os.environ[\"account\"] = str(account)\n",
    "os.environ[\"region\"] = str(region)\n",
    "os.environ[\"bucket_name\"] = str(bucket_name)\n",
    "os.environ[\"base_job_name\"] = str(base_job_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "!bash ./cloud/build_and_push.sh {image} {is_build}\n",
    "os.chdir(\"cloud\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Push Image to ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_build == \"true\":\n",
    "  !docker push $account.dkr.ecr.$region.amazonaws.com/${image}:latest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pack Source Code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This section deals with packing only the necessary code for running on Sagemaker.\n",
    "- We send this code to a predetermined location on S3.\n",
    "- Sagemaker will start the run and download the source code, saving it to the main directory.\n",
    "- During the packing process, it will ignore all cache and hidden files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sagemaker_utils import pack_archive, upload_to_s3, delete_archive\n",
    "current_dir = os.getcwd()  # Get current directory\n",
    "project_dir = os.path.dirname(current_dir)  # Get parent directory\n",
    "\n",
    "source_dir_names = [\n",
    "    \"actors\",\n",
    "    \"cogment_verse\",\n",
    "    \"config\",\n",
    "    \"environments\",\n",
    "    \"runs\",\n",
    "    \"tests\",\n",
    "    \"main.py\",\n",
    "    \"simple_mlflow.py\",\n",
    "]\n",
    "ignore_folders = [\"node_modules\"]\n",
    "archive_name = \"source_code.tar.gz\"\n",
    "\n",
    "# Pack all source code to run cogment verse\n",
    "pack_archive(project_dir=project_dir, \n",
    "             main_dir=current_dir, \n",
    "             output_path=project_dir, \n",
    "             source_dir_names=source_dir_names, \n",
    "             ignore_folders=ignore_folders, \n",
    "             archive_name=archive_name)\n",
    "\n",
    "# Upload to S3\n",
    "s3_key = f\"{image}/input/data/{archive_name}\"\n",
    "print(project_dir)\n",
    "upload_to_s3(local_path=f\"{project_dir}/{archive_name}\", bucket=bucket_name, s3_key=s3_key)\n",
    "\n",
    "# Delete packed source code after uploading to S3\n",
    "delete_archive(archive_path=f\"{project_dir}/{archive_name}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User Inputs\n",
    "- `main_args` is the name experiment i.e., `python -m main <main_args>` (see README.md)\n",
    "- `s3_bucket` is the location where sagemaker instance will push/fetch all relevant data for the run\n",
    "- `repo` is the main folder inside the `s3_bucket` that will store source code as well as model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {'main_args': \"+experiment=ppo_atari_pz/pong_pz\", 's3_bucket': bucket_name, \"repo\": image}\n",
    "run_local_test = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local Test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is important to ensure that the Docker image has been built correctly and can run smoothly on your local machine before deploying it to an AWS instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_local_test:\n",
    "    # Training setup\n",
    "    output_path = f\"s3://{bucket_name}/{image}/output\"\n",
    "    input_path = f\"s3://{bucket_name}/{image}/input/data\"\n",
    "    image_name = f\"{account}.dkr.ecr.{region}.amazonaws.com/{image}:latest\"\n",
    "\n",
    "    estimator = sagemaker.estimator.Estimator(image_uri=image_name,\n",
    "                        base_job_name=base_job_name,\n",
    "                        role=role, \n",
    "                        instance_count=1, \n",
    "                        output_path=output_path,\n",
    "                        instance_type='local',\n",
    "                        hyperparameters=hyperparameters)\n",
    "    estimator.fit(inputs={\"training\": input_path})\n",
    "\n",
    "    # Verification\n",
    "    print(f\"input_path: {input_path}\")\n",
    "    print(f\"output_path: {output_path}\")\n",
    "    print(f\"image_name: {image_name}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AWS Run"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature**\n",
    "- To monitor the progress of your machine learning training with mlflow, run the command `python -m simple_mlflow` in the terminal as usual\n",
    "- Before you finish, make sure to double-check that your Sagemaker training job has ended to avoid any additional charges because sometimes cog-verse does not terminate properly \n",
    "- Model registry folder will be uploaded to S3\n",
    "\n",
    "**Limitation**\n",
    "- We do not have the capability to store historical data for mlflow runs yet. This means that each new run will overwrite the previous run's data\n",
    "- Current setup does not automatically synchronize the model registry from S3 to the local machine. However, users can set up this process according to their needs\n",
    "\n",
    "**Note**\n",
    "- Sagemaker instance will be still running even if users turn off the computer or notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sagemaker_utils import download_and_extract_data_from_s3\n",
    "import time\n",
    "\n",
    "# Training setup\n",
    "output_path = f\"s3://{bucket_name}/{image}/output\"\n",
    "input_path = f\"s3://{bucket_name}/{image}/input/data\"\n",
    "image_name = f\"{account}.dkr.ecr.{region}.amazonaws.com/{image}:latest\"\n",
    "tag_name = [{'Key': 'cog-verse', 'Value': 'cog-verse-training'}]\n",
    "base_job_name = 'cog-verse-training'\n",
    "\n",
    "# Run the sagemaker without waiting \n",
    "estimator = sagemaker.estimator.Estimator(image_uri=image_name,\n",
    "                       base_job_name=base_job_name,\n",
    "                       role=role, \n",
    "                       instance_count=1, \n",
    "                       instance_type=instance_type,\n",
    "                       tags=tag_name,\n",
    "                       output_path=output_path,\n",
    "                       sagemaker_session=sagemaker_session,\n",
    "                       hyperparameters=hyperparameters)\n",
    "estimator.fit(inputs={\"training\": input_path}, wait=False)\n",
    "\n",
    "# Wait for training job start before syncing mlflow data\n",
    "while True:\n",
    "    training_job_info = estimator.latest_training_job.describe()\n",
    "    status = training_job_info['TrainingJobStatus']\n",
    "    if status == 'InProgress':\n",
    "        time.sleep(60)\n",
    "        break\n",
    "\n",
    "# Sync mlflow data from S3 to local machine\n",
    "cwd_dir = os.getcwd()  # Get current directory\n",
    "project_dir = os.path.dirname(current_dir)  # Get parent directory\n",
    "mlflow_archive_name = \"mlflow_db.tar.gz\" # this name is set in sagemaker_main.py\n",
    "mlflow_s3_folder = f\"{image}/mlflow/{mlflow_archive_name}\" # this name is set in sagemaker_main.py\n",
    "unpack_path = f\"{project_dir}/.cogment_verse\"\n",
    "\n",
    "print(\"Syncing mlflow data...\")\n",
    "while True:\n",
    "    # Get training job info\n",
    "    training_job_info = estimator.latest_training_job.describe()\n",
    "\n",
    "    # Stop syncing process when the job is done running\n",
    "    if training_job_info[\"TrainingJobStatus\"] in ['Completed', 'Failed', 'Stopped']:\n",
    "        break\n",
    "\n",
    "    # Sync mlflow data from S3 to local machine\n",
    "    download_and_extract_data_from_s3(bucket=bucket_name, \n",
    "                                    s3_key=mlflow_s3_folder, \n",
    "                                    download_path=cwd_dir, \n",
    "                                    archive_name=mlflow_archive_name, \n",
    "                                    unpack_path=unpack_path)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In situations where the user's computer gets disconnected or turned off, there is a solution available for tracking MLflow metrics. Users can execute the following code to ensure continuous tracking of their MLflow metrics even under such circumstance. Note it will only work if the SageMaker environment is still running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sagemaker_utils import download_and_extract_data_from_s3\n",
    "only_mlflow_sync = False\n",
    "\n",
    "if only_mlflow_sync: \n",
    "  cwd_dir = project_dir = os.getcwd()\n",
    "  mlflow_archive_name = \"mlflow_db.tar.gz\" # this name is set in sagemaker_main.py\n",
    "  mlflow_s3_folder = f\"{image}/mlflow/{mlflow_archive_name}\" # this name is set in sagemaker_main.py\n",
    "  unpack_path = f\"{cwd_dir}/.cogment_verse\"\n",
    "  print(\"Syncing mlflow data...\")\n",
    "  while True:\n",
    "      # Sync mlflow data from S3 to local machine\n",
    "      download_and_extract_data_from_s3(bucket=bucket_name, \n",
    "                                      s3_key=mlflow_s3_folder, \n",
    "                                      download_path=cwd_dir, \n",
    "                                      archive_name=mlflow_archive_name, \n",
    "                                      unpack_path=unpack_path)\n",
    "  print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cog_verse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
