{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS Account Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "try:\n",
    "    # Sagemaker studio\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    # Notebook from local machine\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='Developer')['Role']['Arn']\n",
    "    print(\"Get role successfully\")\n",
    "account = sess.boto_session.client('sts').get_caller_identity()['Account']\n",
    "region = sess.boto_session.region_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build & Push Docker Image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description**\n",
    "- This section deals with the variables related to Docker images that will be pushed to the Elastic Container Registry (ECR) after their build.\n",
    "- Usually, there's no need to build the Docker image more than once because all source codes will be packed and sent to S3 storage.\n",
    "- Any changes made to the source code will not affect the Docker image.\n",
    "- In \"Build Image\" section, `is_build` argument defaults to `False` unless the docker image is not available on ECR or `sagemaker_main.py` is modified \n",
    "\n",
    "**Before Running**\n",
    "- Please change `bucket_name` and don't forget to have docker desktop running"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables for Docker Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloud.sagemaker_utils import create_bucket_if_not_exists\n",
    "image = 'cog_verse'\n",
    "base_job_name = 'cog-verse-training'\n",
    "bucket_name = \"cog-verse\"\n",
    "is_build = \"false\" # whether to build docker\n",
    "instance_type = 'ml.m5.2xlarge'\n",
    "\n",
    "# Create an S3 client\n",
    "create_bucket_if_not_exists(bucket_name=bucket_name, region=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables \n",
    "%env image {image}\n",
    "%env account {account}\n",
    "%env region {region}\n",
    "%env bucket_name {bucket_name}\n",
    "%env base_job_name = {base_job_name}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh -s \"$image\" \"$is_build\"\n",
    "bash ./build_and_push.sh $1 $2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Push Image to ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_build == \"true\":\n",
    "    !docker push $account.dkr.ecr.$region.amazonaws.com/${image}:latest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pack Source Code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This section deals with packing only the necessary code for running on Sagemaker.\n",
    "- We send this code to a predetermined location on S3.\n",
    "- Sagemaker will start the run and download the source code, saving it to the main directory.\n",
    "- During the packing process, it will ignore all cache and hidden files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloud.sagemaker_utils import pack_archive, upload_to_s3, delete_archive\n",
    "project_dir = \".\"\n",
    "source_dir_names = [\n",
    "    \"actors\",\n",
    "    \"cogment_verse\",\n",
    "    \"config\",\n",
    "    \"environments\",\n",
    "    \"runs\",\n",
    "    \"tests\",\n",
    "    \"main.py\",\n",
    "    \"simple_mlflow.py\",\n",
    "]\n",
    "ignore_folders = [\"node_modules\"]\n",
    "archive_name = \"source_code.tar.gz\"\n",
    "\n",
    "# Pack all source code to run cogment verse\n",
    "pack_archive(project_dir=project_dir, \n",
    "             main_dir=project_dir, \n",
    "             output_path=project_dir, \n",
    "             source_dir_names=source_dir_names, \n",
    "             ignore_folders=ignore_folders, \n",
    "             archive_name=archive_name)\n",
    "\n",
    "# Upload to S3\n",
    "s3_key = f\"{image}/input/data/{archive_name}\"\n",
    "upload_to_s3(local_path=f\"./{archive_name}\", bucket=bucket_name, s3_key=s3_key)\n",
    "\n",
    "# Delete packed source code after uploading to S3\n",
    "delete_archive(archive_path=f\"{project_dir}/{archive_name}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {'main_args': \"+experiment=ppo_atari_pz/pong_pz\", 's3_bucket': bucket_name, \"repo\": image}\n",
    "run_local_test = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local Test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is important to ensure that the Docker image has been built correctly and can run smoothly on your local machine before deploying it to an AWS instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_local_test:\n",
    "    # Training setup\n",
    "    output_path = f\"s3://{bucket_name}/{image}/output\"\n",
    "    input_path = f\"s3://{bucket_name}/{image}/input/data\"\n",
    "    image_name = f\"{account}.dkr.ecr.{region}.amazonaws.com/{image}:latest\"\n",
    "\n",
    "    estimator = sagemaker.estimator.Estimator(image_uri=image_name,\n",
    "                        base_job_name=base_job_name,\n",
    "                        role=role, \n",
    "                        instance_count=1, \n",
    "                        output_path=output_path,\n",
    "                        instance_type='local',\n",
    "                        hyperparameters=hyperparameters)\n",
    "    estimator.fit(inputs={\"training\": input_path})\n",
    "\n",
    "    # Verification\n",
    "    print(f\"input_path: {input_path}\")\n",
    "    print(f\"output_path: {output_path}\")\n",
    "    print(f\"image_name: {image_name}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AWS Run"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature**\n",
    "- To monitor the progress of your machine learning training with mlflow, run the command `python -m simple_mlflow` in the terminal as usual\n",
    "- Before you finish, make sure to double-check that your Sagemaker training job has ended to avoid any additional charges because sometimes cog-verse does not terminate properly \n",
    "- Model registry folder will be uploaded to S3\n",
    "\n",
    "**Limitation**\n",
    "- We do not have the capability to store historical data for mlflow runs. This means that each new run will overwrite the previous run's data\n",
    "- Current setup does not automatically synchronize the model registry from S3 to the local machine. However, users can set up this process according to their needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from cloud.sagemaker_utils import download_and_extract_data_from_s3\n",
    "import time\n",
    "\n",
    "# Training setup\n",
    "output_path = f\"s3://{bucket_name}/{image}/output\"\n",
    "input_path = f\"s3://{bucket_name}/{image}/input/data\"\n",
    "image_name = f\"{account}.dkr.ecr.{region}.amazonaws.com/{image}:latest\"\n",
    "tag_name = [{'Key': 'cog-verse', 'Value': 'cog-verse-training'}]\n",
    "base_job_name = 'cog-verse-training'\n",
    "\n",
    "# Run the sagemaker without waiting \n",
    "estimator = sagemaker.estimator.Estimator(image_uri=image_name,\n",
    "                       base_job_name=base_job_name,\n",
    "                       role=role, \n",
    "                       instance_count=1, \n",
    "                       instance_type=instance_type,\n",
    "                       tags=tag_name,\n",
    "                       output_path=output_path,\n",
    "                       sagemaker_session=sess,\n",
    "                       hyperparameters=hyperparameters)\n",
    "estimator.fit(inputs={\"training\": input_path}, wait=False)\n",
    "\n",
    "# Wait for training job start before syncing mlflow data\n",
    "while True:\n",
    "    training_job_info = estimator.latest_training_job.describe()\n",
    "    status = training_job_info['TrainingJobStatus']\n",
    "    if status == 'InProgress':\n",
    "        time.sleep(60)\n",
    "        break\n",
    "\n",
    "# Sync mlflow data from S3 to local machine\n",
    "cwd_dir = project_dir = os.getcwd()\n",
    "mlflow_archive_name = \"mlflow_db.tar.gz\" # this name is set in sagemaker_main.py\n",
    "mlflow_s3_folder = f\"{image}/mlflow/{mlflow_archive_name}\" # this name is set in sagemaker_main.py\n",
    "unpack_path = f\"{cwd_dir}/.cogment_verse\"\n",
    "print(\"Syncing mlflow data...\")\n",
    "while True:\n",
    "    # Get training job info\n",
    "    training_job_info = estimator.latest_training_job.describe()\n",
    "\n",
    "    # Stop syncing process when the job is done running\n",
    "    if training_job_info[\"TrainingJobStatus\"] in ['Completed', 'Failed', 'Stopped']:\n",
    "        break\n",
    "\n",
    "    # Sync mlflow data from S3 to local machine\n",
    "    download_and_extract_data_from_s3(bucket=bucket_name, \n",
    "                                    s3_key=mlflow_s3_folder, \n",
    "                                    download_path=cwd_dir, \n",
    "                                    archive_name=mlflow_archive_name, \n",
    "                                    unpack_path=unpack_path)\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cog_verse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
