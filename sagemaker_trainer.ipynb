{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS Account Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='Developer')['Role']['Arn']\n",
    "    print(\"Get role successfully\")\n",
    "account = sess.boto_session.client('sts').get_caller_identity()['Account']\n",
    "region = sess.boto_session.region_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build & Push Docker Image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This section deals with the variables related to Docker images that will be pushed to the Elastic Container Registry (ECR) after their build.\n",
    "- Usually, there's no need to build the Docker image more than once because all source codes will be packed and sent to S3 storage.\n",
    "- Any changes made to the source code will not affect the Docker image.\n",
    "- \"Build Image\" section, we should set the `is_build` argument to `False` unless we want to build and push the image during every run.\n",
    "- Please change the `image name` "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables for Docker Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = 'cog_verse'\n",
    "bucket_name   = sess.default_bucket()\n",
    "base_job_name = 'cog-verse-training'\n",
    "%env image {image}\n",
    "%env account {account}\n",
    "%env region {region}\n",
    "%env bucket_name {bucket_name}\n",
    "%env base_job_name = {base_job_name}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable/disable the docker build\n",
    "is_build = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh -s \"$image\" \"$is_build\"\n",
    "bash ./build_and_push.sh $1 $2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Push Image to ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_build == \"true\":\n",
    "    !docker push $account.dkr.ecr.$region.amazonaws.com/${image}:latest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pack Source Code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This section deals with packing only the necessary code for running on Sagemaker.\n",
    "- We send this code to a predetermined location on S3.\n",
    "- Sagemaker will start the run and download the source code, saving it to the main directory.\n",
    "- During the packing process, it will ignore all cache and dot files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloud.sagemaker_utils import pack_archive, upload_to_s3, delete_archive\n",
    "project_dir = \".\"\n",
    "source_dir_names = [\n",
    "    \"actors\",\n",
    "    \"cogment_verse\",\n",
    "    \"config\",\n",
    "    \"environments\",\n",
    "    \"runs\",\n",
    "    \"tests\",\n",
    "    \"main.py\",\n",
    "    \"simple_mlflow.py\",\n",
    "]\n",
    "ignore_folders = [\"node_modules\"]\n",
    "archive_name = \"source_code.tar.gz\"\n",
    "\n",
    "# Pack all source code to run cogment verse\n",
    "pack_archive(project_dir=project_dir, main_dir=project_dir, output_path=project_dir,source_dir_names=source_dir_names, ignore_folders=ignore_folders, archive_name=archive_name)\n",
    "\n",
    "# Upload to S3\n",
    "s3_key = f\"{image}/input/data/{archive_name}\"\n",
    "upload_to_s3(local_path=f\"./{archive_name}\", bucket=bucket_name, s3_key=s3_key)\n",
    "\n",
    "# Delete packed source code after uploading to S3\n",
    "delete_archive(archive_path=f\"{project_dir}/{archive_name}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {'main_args': \"+experiment=ppo_atari_pz/pong_pz\", 's3_bucket': bucket_name, \"repo\": image}\n",
    "run_local_test = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_local_test:\n",
    "    # Training setup\n",
    "    output_path = f\"s3://{bucket_name}/{image}/output\"\n",
    "    input_path = f\"s3://{bucket_name}/{image}/input/data\"\n",
    "    image_name = f\"{account}.dkr.ecr.{region}.amazonaws.com/{image}:latest\"\n",
    "\n",
    "    estimator = sagemaker.estimator.Estimator(image_uri=image_name,\n",
    "                        base_job_name=base_job_name,\n",
    "                        role=role, \n",
    "                        instance_count=1, \n",
    "                        output_path=output_path,\n",
    "                        instance_type='local',\n",
    "                        hyperparameters=hyperparameters)\n",
    "    estimator.fit(inputs={\"training\": input_path})\n",
    "\n",
    "    # Verification\n",
    "    print(f\"input_path: {input_path}\")\n",
    "    print(f\"output_path: {output_path}\")\n",
    "    print(f\"image_name: {image_name}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AWS Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloud.sagemaker_utils import download_and_extract_data_from_s3\n",
    "# Training setup\n",
    "output_path = f\"s3://{bucket_name}/{image}/output\"\n",
    "input_path = f\"s3://{bucket_name}/{image}/input/data\"\n",
    "image_name = f\"{account}.dkr.ecr.{region}.amazonaws.com/{image}:latest\"\n",
    "tag_name = [{'Key': 'cog-verse', 'Value': 'cog-verse-training'}]\n",
    "base_job_name = 'cog-verse-training'\n",
    "\n",
    "# Run the sagemaker without waiting \n",
    "estimator = sagemaker.estimator.Estimator(image_uri=image_name,\n",
    "                       base_job_name=base_job_name,\n",
    "                       role=role, \n",
    "                       instance_count=1, \n",
    "                       instance_type='ml.m5.xlarge',\n",
    "                       tags=tag_name,\n",
    "                       output_path=output_path,\n",
    "                       sagemaker_session=sess,\n",
    "                       hyperparameters=hyperparameters)\n",
    "estimator.fit(inputs={\"training\": input_path}, wait=True)\n",
    "\n",
    "# # Sync mlflow data from S3 to local machine\n",
    "# mlflow_archive_name = \"mlflow_db.tar.gz\" # this name is set in sagemaker_main.py\n",
    "# mlflow_s3_folder = f\"{image}/mlflow/{mlflow_archive_name}\" # this name is set in sagemaker_main.py\n",
    "# local_path = f\".cogment_verse/mlflow/{mlflow_archive_name}\"\n",
    "# while True:\n",
    "#     # Get training job info\n",
    "#     training_job_info = estimator.latest_training_job.describe()\n",
    "\n",
    "#     # Stop syncing process when the job is done running\n",
    "#     if training_job_info[\"TrainingJobStatus\"] in ['Completed', 'Failed', 'Stopped']:\n",
    "#         break\n",
    "\n",
    "#     # Sync mlflow data from S3 to local machine\n",
    "#     download_and_extract_data_from_s3(bucket=bucket_name, s3_key=mlflow_s3_folder, local_path=local_path)\n",
    "\n",
    "\n",
    "# Verification\n",
    "print(f\"output_path: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cog_verse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
