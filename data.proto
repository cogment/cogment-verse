// Copyright 2021 AI Redefined Inc. <dev+cogment@ai-r.com>
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//    http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

syntax = "proto3";

package cogment_verse;

message NDArray {
  string dtype = 1;
  repeated uint32 shape = 2;
  bytes data = 3;
}

message EnvironmentConfig {
  string run_id = 1;
  bool render = 5;
  bool flatten = 6;
  int32 render_width = 7;
  uint32 framestack = 8;
  uint32 seed = 9;
  string mode = 10;
}

message EnvironmentSpecs {
  string implementation = 1;
  int32 num_players = 2;
  int32 num_input = 3;
  int32 num_action = 4;
}

message EnvironmentParams {
  EnvironmentSpecs specs = 2; // Keeping that here for the moment, could probably be defined somewhere else with only the implementation name defined in the params
  EnvironmentConfig config = 3;
}

message AgentConfig {
  string run_id = 1;
  EnvironmentSpecs environment_specs = 2;
  string model_id = 3;
  int32 model_version = 4;
  int32 actor_index = 5; // Used to figure out if an agent is the current_player in the observation space
  string device = 6;
  uint32 threads_per_worker = 7;
  string repo_id = 8;
  string filename = 9;
}

enum HumanRole {
  TEACHER = 0;
  OBSERVER = 1;
}

message HumanConfig {
  string run_id = 1;
  EnvironmentSpecs environment_specs = 2;
  int32 actor_index = 3; // Used to figure out if an agent is the current_player in the observation space
  HumanRole role = 4;
}

message ActorParams {
  string name = 1;
  string actor_class = 2;
  string implementation = 3;
  oneof config_oneof {
    AgentConfig agent_config = 4;
    HumanConfig human_config = 5;
  }
}

message TrialConfig {
  string run_id = 1;
  EnvironmentParams environment = 3;
  repeated ActorParams actors = 4;
  int32 distinguished_actor = 6;
}

message Observation {
  NDArray vectorized = 1;
  bytes pixel_data = 2;
  repeated int32 legal_moves_as_int = 3;
  int32 current_player = 4; // active player for multi-agent turn-based environments
  int32 player_override = 5;  // player that _actually_ acted (in case of override/intervention)
  NDArray segmentation = 6;
}

message ContinuousAction {
  repeated float data = 1;
}

message AgentAction {
  oneof action {
      ContinuousAction continuous_action = 1;
      int32 discrete_action = 2;
  };
  NDArray policy = 3; // optional: policy from which action was drawn
  float value = 4; // optional: value of the state from which the action was taken
}

message ModelArgs {
  float v_min = 1;
  float v_max = 2;
  uint32 start_timesteps = 4;
  repeated float high_action = 5;
  repeated float low_action = 6;
  float expl_noise = 7;
  bool target_net_soft_update = 8;
  uint32 screensize = 9;
}

message ReplayBufferConfig {
  string action_dtype = 1;
  string observation_dtype = 2;
}

message RunConfig {
  EnvironmentParams environment = 1;
  string agent_implementation = 2;
  float epsilon_min = 3;
  uint32 epsilon_steps = 4;
  uint32 target_net_update_schedule = 5;
  float learning_rate = 6;
  uint32 lr_warmup_steps = 7;
  uint32 demonstration_count = 8;
  uint32 total_trial_count = 9;
  uint32 model_publication_interval = 10;
  uint32 model_archive_interval = 11;
  uint32 batch_size = 12;
  uint32 min_replay_buffer_size = 13;
  uint32 max_parallel_trials = 14;
  ModelArgs model_kwargs = 15;
  uint32 max_replay_buffer_size = 16;
  bool aggregate_by_actor = 17;
  ReplayBufferConfig replay_buffer_config = 18;
  float discount_factor = 19;
}

message MLPNetworkConfig {
  uint32 input_size = 1;
  uint32 hidden_size = 2;
  uint32 num_hidden_layers = 3;
  uint32 output_size = 4;
}

message SimpleA2CTrainingConfig {
  uint32 epoch_count = 1;
  uint32 epoch_trial_count = 2;
  uint32 max_parallel_trials = 3;
  float discount_factor = 4;
  float entropy_coef = 5;
  float value_loss_coef = 6;
  float action_loss_coef = 7;
  float learning_rate = 8;
}

message SimpleA2CTrainingRunConfig {
  EnvironmentParams environment = 1;
  SimpleA2CTrainingConfig training = 2;
  MLPNetworkConfig actor_network = 3;
  MLPNetworkConfig critic_network = 4;
}

message DistributionConfig {
  float min_value = 1;
  float max_value = 2;
  uint32 num_bins = 3;
}

message MCTSConfig {
  uint32 max_depth = 1;
  uint32 num_samples = 2;
  float temperature = 3;
  float ucb_c1 = 4;
  float ucb_c2 = 5;
  float exploration_alpha = 6;
  float exploration_epsilon = 7;
  uint32 rollout_length = 8;
  float epsilon_min = 9;
  float epsilon_decay_steps = 10;
  float min_temperature = 11;
  uint32 temperature_decay_steps = 12;
}

message OptimizerConfig {
  float learning_rate = 1;
  float weight_decay = 2;
  float min_learning_rate = 3;
  uint32 lr_warmup_steps = 4;
  uint32 lr_decay_steps = 5;
  float max_norm = 6;
}

message MuZeroTrainingConfig {
  uint32 model_publication_interval = 1;
  float discount_rate = 2;
  OptimizerConfig optimizer = 3;
  uint32 bootstrap_steps = 4;
  uint32 batch_size = 5;
  uint32 max_replay_buffer_size = 6;
  uint32 min_replay_buffer_size = 7;
  uint32 log_interval = 8;
  float similarity_weight = 9;
  float value_weight = 10;
}

message MuZeroRunConfig {
  EnvironmentParams environment = 1;
  AgentConfig actor = 2;
  MuZeroTrainingConfig training = 3;
  MCTSConfig mcts = 4;
  MLPNetworkConfig representation_network = 5;
  MLPNetworkConfig projector_network = 6;
  MLPNetworkConfig dynamics_network = 7;
  MLPNetworkConfig policy_network = 8;
  MLPNetworkConfig value_network = 9;
  DistributionConfig reward_distribution = 10;
  DistributionConfig value_distribution = 11;
  uint32 trial_count = 12;
  uint32 max_parallel_trials = 13;
  uint32 demonstration_trials = 14;
  string train_device = 15;
  string actor_device = 16;
  string reanalyze_device = 17;
  uint32 reanalyze_workers = 18;
  uint32 threads_per_worker = 19;
}

message SimpleBCTrainingConfig {
  uint32 trial_count = 2;
  uint32 max_parallel_trials = 3;
  float discount_factor = 4;
  float learning_rate = 8;
  uint32 batch_size = 5;
}

message SimpleBCTrainingRunConfig {
  EnvironmentParams environment = 1;
  SimpleBCTrainingConfig training = 2;
  MLPNetworkConfig policy_network = 3;
}

message PlayRunConfig {
  EnvironmentParams environment = 1;
  repeated ActorParams actors = 2;
  bool observer = 3;
  uint32 trial_count = 4;
}

message SimpleSB3TrainingConfig {
  uint32 trial_count = 2;
  uint32 max_parallel_trials = 3;
  float discount_factor = 4;
  float learning_rate = 8;
  uint32 batch_size = 5;
}

message SimpleSB3ModelConfig {
  string repo_id = 1;
  string file_name = 2;
}

message SimpleSB3TrainingRunConfig {
  EnvironmentParams environment = 1;
  SimpleSB3TrainingConfig training = 2;
  SimpleSB3ModelConfig model_load = 3;
}

message SelfPlayActorConfig{
  uint32 num_input = 1;
  uint32 num_input_2 = 2;
  uint32 num_action = 3;
  MLPNetworkConfig actor_network = 4;
  MLPNetworkConfig critic_network = 5;
  ModelArgs model_kwargs = 6;
  string run_id = 7;
  string model_id = 8;
  int32 model_version = 9;
  string environment_implementation = 10;
  repeated float action_scale = 11;
  repeated float action_bias = 12;
  uint32 max_action = 13;
  repeated uint32 alice_grid_shape = 14;
  repeated uint32 bob_grid_shape = 15;
}

message SelfPlayRolloutConfig{
  uint32 epoch_count = 1;
  uint32 epoch_train_trial_count = 2;
  uint32 epoch_test_trial_count = 3;
  uint32 max_parallel_trials = 4;
  uint32 model_publication_interval = 5;
  uint32 number_turns_per_trial = 6;
  uint32 test_freq = 7;
}

message SelfPlayReplaybufferConfig{
  uint32 min_replay_buffer_size = 1;
  uint32 max_replay_buffer_size = 2;
  ReplayBufferConfig replay_buffer_config = 3;
}

message SelfPlayTrainingConfig{
  float discount_factor = 1;
  float tau = 2;
  float policy_noise = 3;
  float noise_clip = 4;
  float learning_rate = 5;
  uint32 policy_freq = 6;
  uint32 batch_size = 7;
  float SIGMA = 8;
  uint32 num_training_steps = 9;
  float beta = 10;
  float alice_reward = 11;
  float bob_reward = 12;
  float alice_penalty = 13;
  float bob_penalty = 14;
}


message SelfPlayActorParams{
  string name = 1;
  string actor_class = 2;
  string implementation = 3;
  SelfPlayActorConfig config = 4;
}

message SelfPlayTD3TrainingRunConfig {
  EnvironmentParams environment = 1;
  SelfPlayActorParams actor = 2;
  SelfPlayRolloutConfig rollout = 3;
  SelfPlayTrainingConfig training = 4;
  SelfPlayReplaybufferConfig replaybuffer = 5;
}
