
cartpole_rainbow: &default_params
  implementation: "cogment_verse_run_impl"
  config: &default_config
    class_name: data_pb2.RunConfig
    environment:
      specs:
        implementation: gym/CartPole-v0
        num_input: 4
        num_action: 2
      config: &default_env_config
        player_count: 1
        render_width: 256
        flatten: True
        framestack: 1
    epsilon_min: 0.1
    epsilon_steps: 100000
    target_net_update_schedule: 1000
    learning_rate: 1.0e-4
    lr_warmup_steps: 10000
    demonstration_count: 0
    total_trial_count: 10000
    model_publication_interval: 1000
    model_archive_interval_multiplier: 4 # Archive every fourth published model
    batch_size: 256
    min_replay_buffer_size: 1000
    max_parallel_trials: 4
    model_kwargs: {}
    max_replay_buffer_size: 100000
    aggregate_by_actor: False
    replay_buffer_config:
      observation_dtype: float32
      action_dtype: int8
    agent_implementation: rainbowtorch

cartpole_dqn:
  <<: *default_params
  config:
    <<: *default_config
    agent_implementation: dqn

cartpole_REINFORCE:
  implementation: "reinforce_training"
  config:
    <<: *default_config
    agent_implementation: reinforce
    min_replay_buffer_size: 100000
    max_parallel_trials: 1
    learning_rate: 3.0e-4
    discount_factor: 0.99

pendulum_td3:
  <<: *default_params
  config:
    <<: *default_config
    environment:
      specs:
        implementation: gym/Pendulum-v0
        num_input: 3
        num_action: 1
      config: *default_env_config
    agent_implementation: td3
    epsilon_min: 0.0
    epsilon_steps: 1
    target_net_update_schedule: 2
    learning_rate: 3.0e-4
    lr_warmup_steps: 100
    demonstration_count: 0
    #total_trial_count: 1000
    total_trial_count: 1000
    model_publication_interval: 2500
    min_replay_buffer_size: 256
    batch_size: 256
    replay_buffer_config:
      observation_dtype: float32
      action_dtype: float32

minatar_breakout_rainbow:
  <<: *default_params
  config:
    <<: *default_config
    environment:
      specs:
        implementation: min/breakout
        num_input: 500
        num_action: 6
      config: *default_env_config
    agent_implementation: rainbowtorch

atari_pitfall_rainbow:
  <<: *default_params
  config:
    <<: *default_config
    environment:
      specs:
        implementation: ata/Pitfall
        num_input: 7056
        num_action: 18
      config: *default_env_config
    agent_implementation: rainbowtorch

atari_breakout_cnn:
  <<: *default_params
  config:
    <<: *default_config
    environment:
      specs:
        implementation: ata/Breakout
        num_input: 7056
        num_action: 4
      config:
        <<: *default_env_config
        flatten: false
        framestack: 4
    agent_implementation: atari_cnn
    demonstration_count: 0
    total_trial_count: 10000
    model_publication_interval: 1000
    model_archive_interval_multiplier: 100
    max_parallel_trials: 2
    batch_size: 1024
    min_replay_buffer_size: 10000
    max_replay_buffer_size: 350000
    target_net_update_schedule: 100
    epsilon_steps: 100000
    learning_rate: 1.0e-4
    lr_warmup_steps: 10000
    model_kwargs:
      target_net_soft_update: False
    replay_buffer_config:
      observation_dtype: uint8
      action_dtype: int8

minatar_breakout_demo:
  <<: *default_params
  config:
    <<: *default_config
    environment:
      specs:
        implementation: min/breakout
        num_input: 400
        num_action: 6
      config: *default_env_config
    agent_implementation: rainbowtorch

atari_breakout_demo:
  <<: *default_params
  config:
    <<: *default_config
    environment:
      specs:
        implementation: ata/Breakout
        num_input: 7056
        num_action: 4
      config:
        <<: *default_env_config
        framestack: 4
    agent_implementation: rainbowtorch

atari_pitfall_demo:
  <<: *default_params
  config:
    <<: *default_config
    environment:
      specs:
        implementation: ata/Pitfall
        num_input: 7056
        num_action: 18
      config: *default_env_config
    agent_implementation: rainbowtorch
    demonstration_count: 10

lander_rainbow_demo: &lander_rainbow_demo
  <<: *default_params
  config: &lander_rainbow_config
    <<: *default_config
    environment:
      specs:
        implementation: gym/LunarLander-v2
        num_input: 8
        num_action: 4
      config:
        <<: *default_env_config
        render_width: 1000
    agent_implementation: rainbowtorch
    demonstration_count: 20000 #100
    total_trial_count: 20000
    epsilon_min: 0.1
    epsilon_steps: 10 #1
    target_net_update_schedule: 10
    learning_rate: 1.0e-4
    lr_warmup_steps: 100
    model_publication_interval: 200
    batch_size: 32
    min_replay_buffer_size: 32

lander_rainbow_demo_gpu:
  <<: *lander_rainbow_demo
  config:
    <<: *lander_rainbow_config
    agent_implementation: rainbowtorch
    model_kwargs:
      v_min: -200.0
      v_max: 200.0

tetris_rainbow_demo:
  <<: *default_params
  config:
    <<: *default_config
    environment:
      specs:
        implementation: tet/TetrisA-v0
        num_input: 7056
        num_action: 12
      config:
        <<: *default_env_config
        render_width: 1000
        flatten: False
    agent_implementation: atari_cnn
    demonstration_count: 1000 #100
    total_trial_count: 10000
    epsilon_min: 0.1
    epsilon_steps: 10 #1
    target_net_update_schedule: 10
    learning_rate: 1.0e-4
    lr_warmup_steps: 100
    model_publication_interval: 2000
    batch_size: 32
    min_replay_buffer_size: 32
    max_replay_buffer_size: 10000

connect_four_rainbow:
  <<: *default_params
  config:
    <<: *default_config
    environment:
      specs:
        implementation: pet/connect_four_v3
        num_input: 84
        num_action: 7
      config:
        <<: *default_env_config
        player_count: 2
    agent_implementation: rainbowtorch
    demonstration_count: 0 #100
    total_trial_count: 10000
    epsilon_steps: 5000
    model_publication_interval: 100
    aggregate_by_actor: True
    model_kwargs:
      v_min: -1.0
      v_max: 1.0

backgammon_rainbow:
  <<: *default_params
  config:
    <<: *default_config
    environment:
      specs:
        implementation: pet/backgammon_v3
        num_input: 198
        num_action: 1353
      config:
        <<: *default_env_config
        player_count: 2
    demonstration_count: 0 #100
    total_trial_count: 10
    model_publication_interval: 100

benchmark_lander:
  <<: *default_params
  config:
    <<: *default_config
    environment:
      specs:
        implementation: gym/LunarLander-v2
        num_input: 8
        num_action: 4
      config:
        <<: *default_env_config
        render_width: 1000
    agent_implementation: rainbowtorch
    demonstration_count: 0
    total_trial_count: 250
    epsilon_min: 0.10
    epsilon_steps: 10000 #1
    target_net_update_schedule: 10
    learning_rate: 1.0e-4
    lr_warmup_steps: 100
    model_publication_interval: 250
    batch_size: 32
    min_replay_buffer_size: 32
    model_kwargs:
      v_min: -200.0
      v_max: 200.0

benchmark_lander_hill:
  <<: *default_params
  config:
    <<: *default_config
    environment:
      specs:
        implementation: gym/LunarLander-v2
        num_input: 8
        num_action: 4
      config:
        <<: *default_env_config
        render_width: 1000
    agent_implementation: rainbowtorch
    demonstration_count: 1000
    total_trial_count: 1000
    epsilon_min: 0.1
    epsilon_steps: 1000 #1
    target_net_update_schedule: 10
    learning_rate: 1.0e-4
    lr_warmup_steps: 100
    model_publication_interval: 250
    batch_size: 256
    min_replay_buffer_size: 256
    model_kwargs:
      v_min: -200.0
      v_max: 200.0

lander_continuous_td3:
  <<: *default_params
  config:
    <<: *default_config
    environment:
      specs:
        implementation: gym/LunarLanderContinuous-v2
        num_input: 8
        num_action: 2
      config:
        <<: *default_env_config
        render_width: 1000
    agent_implementation: td3
    demonstration_count: 0
    total_trial_count: 1000
    epsilon_min: 0.0
    epsilon_steps: 1
    target_net_update_schedule: 2
    learning_rate: 1.0e-4
    lr_warmup_steps: 100
    model_publication_interval: 250
    render_width: 1000
    batch_size: 256
    min_replay_buffer_size: 256
    max_replay_buffer_size: 100000
    max_parallel_trials: 8
    replay_buffer_config:
      observation_dtype: float32
      action_dtype: float32
    model_kwargs:
      start_timesteps: 25000
      high_action: [1, 1]
      low_action: [-1, -1]
      expl_noise: 0.1

lander_continuous_ddpg:
  <<: *default_params
  config:
    <<: *default_config
    environment:
      specs:
        implementation: gym/LunarLanderContinuous-v2
        num_input: 8
        num_action: 2
      config:
        <<: *default_env_config
        render_width: 1000
    agent_implementation: ddpg
    demonstration_count: 0
    total_trial_count: 1000
    epsilon_min: 0.1
    epsilon_steps: 30000
    target_net_update_schedule: 2
    learning_rate: 1.0e-4
    lr_warmup_steps: 100
    model_publication_interval: 250
    batch_size: 64
    min_replay_buffer_size: 64
    max_replay_buffer_size: 100000
    max_parallel_trials: 8
    replay_buffer_config:
      observation_dtype: float32
      action_dtype: float32
    model_kwargs:
      start_timesteps: 25000
      high_action: [1, 1]
      low_action: [-1, -1]
      expl_noise: 0.1

walker_td3:
  <<: *default_params
  config:
    <<: *default_config
    environment:
      specs:
        implementation: gym/BipedalWalker-v3
        num_input: 8
        num_action: 2
      config:
        <<: *default_env_config
        render_width: 1000
    agent_implementation: td3
    demonstration_count: 0
    total_trial_count: 1000
    epsilon_min: 0.0
    epsilon_steps: 1
    target_net_update_schedule: 2
    learning_rate: 1.0e-4
    lr_warmup_steps: 100
    model_publication_interval: 250
    batch_size: 256
    min_replay_buffer_size: 256
    max_replay_buffer_size: 100000
    max_parallel_trials: 8
    replay_buffer_config:
      observation_dtype: float32
      action_dtype: float32
    model_kwargs:
      start_timesteps: 25000
      high_action: [1, 1, 1, 1]
      low_action: [-1, -1, -1, -1]
      expl_noise: 0.1

walker_ddpg:
  <<: *default_params
  config:
    <<: *default_config
    environment:
      specs:
        implementation: gym/BipedalWalker-v3
        num_input: 24
        num_action: 4
      config:
        <<: *default_env_config
        render_width: 1000
    agent_implementation: ddpg
    demonstration_count: 0
    total_trial_count: 1000
    epsilon_min: 0.1
    epsilon_steps: 50000
    target_net_update_schedule: 2
    learning_rate: 1.0e-4
    model_publication_interval: 250
    batch_size: 64
    min_replay_buffer_size: 64
    max_replay_buffer_size: 100000
    replay_buffer_config:
      observation_dtype: float32
      action_dtype: float32
    model_kwargs:
      start_timesteps: 25000
      high_action: [1, 1, 1, 1]
      low_action: [-1, -1, -1, -1]
      expl_noise: 0.1


pendulum_td3:
  <<: *default_params
  config:
    <<: *default_config
    environment:
      specs:
        implementation: gym/Pendulum-v0
        num_input: 3
        num_action: 1
      config:
        <<: *default_env_config
        render_width: 1000
    agent_implementation: td3
    demonstration_count: 0 #100
    total_trial_count: 250
    epsilon_min: 0.0
    epsilon_steps: 1
    target_net_update_schedule: 2
    #learning_rate: 3.0e-4
    learning_rate: 1.0e-4
    lr_warmup_steps: 100
    model_publication_interval: 1000
    batch_size: 64
    min_replay_buffer_size: 64
    replay_buffer_config:
      observation_dtype: float32
      action_dtype: float32
    model_kwargs:
      start_timesteps: 5000
      high_action: [2]
      low_action: [-2]
      expl_noise: 0.1

pendulum_ddpg:
  <<: *default_params
  config:
    <<: *default_config
    environment:
      specs:
        implementation: gym/Pendulum-v0
        num_input: 3
        num_action: 1
      config:
        <<: *default_env_config
        render_width: 1000
    agent_implementation: ddpg
    demonstration_count: 0 #100
    total_trial_count: 250
    epsilon_min: 0.0
    epsilon_steps: 1
    target_net_update_schedule: 2
    #learning_rate: 3.0e-4
    learning_rate: 1.0e-4
    lr_warmup_steps: 100
    model_publication_interval: 1000
    batch_size: 64
    min_replay_buffer_size: 64
    max_replay_buffer_size: 10000
    replay_buffer_config:
      observation_dtype: float32
      action_dtype: float32
    model_kwargs:
      start_timesteps: 5000
      high_action: [2]
      low_action: [-2]
      expl_noise: 0.1

benchmark_breakout:
  <<: *default_params
  config:
    <<: *default_config
    environment:
      specs:
        implementation: min/breakout
        num_input: 400
        num_action: 6
      config: *default_env_config
    agent_implementation: rainbowtorch
    demonstration_count: 0
    total_trial_count: 1000
    model_publication_interval: 100
    model_kwargs:
      v_min: -1.0
      v_max: 1.0
    batch_size: 32
    min_replay_buffer_size: 32

benchmark_connect_four:
  <<: *default_params
  config:
    <<: *default_config
    environment:
      specs:
        implementation: pet/connect_four_v3
        num_input: 84
        num_action: 7
      config:
        <<: *default_env_config
        player_count: 2
    agent_implementation: rainbowtorch
    environment_implementation:
    demonstration_count: 0 #100
    total_trial_count: 10
    epsilon_steps: 5000
    model_publication_interval: 100
    aggregate_by_actor: True
    model_kwargs:
      v_min: -1.0
      v_max: 1.0
    batch_size: 32
    min_replay_buffer_size: 32

simple_a2c_cartpole: &default_simple_a2c_params
  implementation: simple_a2c_training
  config: &default_simple_a2c_config
    class_name: data_pb2.SimpleA2CTrainingRunConfig
    environment:
      specs:
        implementation: gym/CartPole-v0
        num_input: 4
        num_action: 2
      config:
        <<: *default_env_config
        seed: 12
    training:
      epoch_count: 100
      epoch_trial_count: 15
      max_parallel_trials: 8
      discount_factor: 0.95
      entropy_coef: 0.01
      value_loss_coef: 0.5
      action_loss_coef: 1.0
      learning_rate: 0.01
    actor_network:
      hidden_size: 64
    critic_network:
      hidden_size: 64


# procgen environments follow
procgen_bigfish:
  <<: *default_params
  config:
    <<: *default_config
    environment:
      specs:
        implementation: procgen/bigfish
        num_input: 7056
        num_action: 15
      config:
        <<: *default_env_config
        render_width: 64
        flatten: false
        framestack: 4
    agent_implementation: atari_cnn
    demonstration_count: 100
    total_trial_count: 10000
    model_publication_interval: 1000
    model_archive_interval_multiplier: 10
    max_parallel_trials: 2
    batch_size: 32
    min_replay_buffer_size: 10000
    max_replay_buffer_size: 350000
    target_net_update_schedule: 100
    epsilon_steps: 100000
    learning_rate: 1.0e-4
    lr_warmup_steps: 10000
    model_kwargs:
      target_net_soft_update: False
      screensize: 64
    replay_buffer_config:
      observation_dtype: uint8
      action_dtype: int8


simple_bc_lander: &simple_bc_experiment
  implementation: simple_bc_training
  config: &simple_bc_config
    class_name: data_pb2.SimpleBCTrainingRunConfig
    environment:
      specs:
        implementation: gym/LunarLander-v2
        num_input: 8
        num_action: 4
      config:
        seed: 12
    training:
      trial_count: 1000
      max_parallel_trials: 1
      learning_rate: 0.001
      batch_size: 32
    policy_network:
      hidden_size: 64

simple_bc_cartpole:
  <<: *simple_bc_experiment
  implementation: simple_bc_training
  config:
    <<: *simple_bc_config
    environment:
      specs:
        implementation: gym/CartPole-v0
        num_input: 4
        num_action: 2
      config:
        seed: 12

simple_bc_mountaincar:
  <<: *simple_bc_experiment
  implementation: simple_bc_training
  config:
    <<: *simple_bc_config
    environment:
      specs:
        implementation: gym/MountainCar-v0
        num_input: 2
        num_action: 3
      config:
        seed: 12

