
cartpole_rainbow: &default_params
  implementation: "cogment_verse_run_impl"
  config: &default_config
    class_name: data_pb2.RunConfig
    player_count: 1
    epsilon_min: 0.1
    epsilon_steps: 100000
    target_net_update_schedule: 1000
    learning_rate: 1.0e-4
    lr_warmup_steps: 10000
    demonstration_count: 0
    total_trial_count: 10000
    model_publication_interval: 1000
    model_archive_interval_multiplier: 4 # Archive every fourth published model
    render_width: 256
    batch_size: 256
    min_replay_buffer_size: 1000
    max_parallel_trials: 4
    model_kwargs: {}
    max_replay_buffer_size: 100000
    flatten: True
    aggregate_by_actor: False
    framestack: 1
    replay_buffer_config:
      observation_dtype: float32
      action_dtype: int8
    num_input: 4
    num_action: 2
    agent_implementation: rainbowtorch
    environment_implementation: gym/CartPole-v0

cartpole_dqn:
  <<: *default_params
  config:
    <<: *default_config
    num_input: 4
    num_action: 2
    agent_implementation: dqn
    environment_implementation: gym/CartPole-v0

cartpole_REINFORCE:
  implementation: "reinforce_training"
  config:
    <<: *default_config
    agent_implementation: reinforce
    min_replay_buffer_size: 100000
    max_parallel_trials: 1
    learning_rate: 3.0e-4
    discount_factor: 0.99

pendulum_td3:
  <<: *default_params
  config:
    <<: *default_config
    num_input: 3
    num_action: 1
    agent_implementation: td3
    environment_implementation: gym/Pendulum-v0
    epsilon_min: 0.0
    epsilon_steps: 1
    target_net_update_schedule: 2
    learning_rate: 3.0e-4
    lr_warmup_steps: 100
    demonstration_count: 0
    #total_trial_count: 1000
    total_trial_count: 1000
    model_publication_interval: 2500
    min_replay_buffer_size: 256
    batch_size: 256
    replay_buffer_config:
      observation_dtype: float32
      action_dtype: float32

minatar_breakout_rainbow:
  <<: *default_params
  config:
    <<: *default_config
    num_input: 400
    num_action: 6
    agent_implementation: rainbowtorch
    environment_implementation: min/breakout

atari_pitfall_rainbow:
  <<: *default_params
  config:
    <<: *default_config
    num_input: 7056
    num_action: 18
    agent_implementation: rainbowtorch
    environment_implementation: ata/Pitfall

atari_breakout_cnn:
  <<: *default_params
  config:
    <<: *default_config
    num_input: 7056
    num_action: 4
    agent_implementation: atari_cnn
    environment_implementation: ata/Breakout
    demonstration_count: 0
    total_trial_count: 10000
    model_publication_interval: 1000
    model_archive_interval_multiplier: 100
    max_parallel_trials: 2
    render_width: 256
    flatten: false
    batch_size: 1024
    min_replay_buffer_size: 10000
    max_replay_buffer_size: 350000
    target_net_update_schedule: 100
    epsilon_steps: 100000
    learning_rate: 1.0e-4
    lr_warmup_steps: 10000
    framestack: 4
    model_kwargs:
      target_net_soft_update: False
    replay_buffer_config:
      observation_dtype: uint8
      action_dtype: int8

minatar_breakout_demo:
  <<: *default_params
  config:
    <<: *default_config
    num_input: 400
    num_action: 6
    agent_implementation: rainbowtorch
    environment_implementation: min/breakout
    demonstration_count: 0

atari_breakout_demo:
  <<: *default_params
  config:
    <<: *default_config
    num_input: 7056
    num_action: 4
    agent_implementation: rainbowtorch
    environment_implementation: ata/Breakout
    demonstration_count: 0
    framestack: 4

atari_pitfall_demo:
  <<: *default_params
  config:
    <<: *default_config
    num_input: 7056
    num_action: 18
    agent_implementation: rainbowtorch
    environment_implementation: ata/Pitfall
    demonstration_count: 10

lander_rainbow_demo: &lander_rainbow_demo
  <<: *default_params
  config: &lander_rainbow_config
    <<: *default_config
    num_input: 8
    num_action: 4
    agent_implementation: rainbowtorch
    environment_implementation: gym/LunarLander-v2
    demonstration_count: 20000 #100
    total_trial_count: 20000
    epsilon_min: 0.1
    epsilon_steps: 10 #1
    target_net_update_schedule: 10
    learning_rate: 1.0e-4
    lr_warmup_steps: 100
    model_publication_interval: 200
    render_width: 1000
    batch_size: 32
    min_replay_buffer_size: 32

lander_rainbow_demo_gpu:
  <<: *lander_rainbow_demo
  config:
    <<: *lander_rainbow_config
    agent_implementation: rainbowtorch
    model_kwargs:
      v_min: -200.0
      v_max: 200.0

tetris_rainbow_demo:
  <<: *default_params
  config:
    <<: *default_config
    num_input: 7056
    num_action: 12
    agent_implementation: atari_cnn
    environment_implementation: tet/TetrisA-v0
    demonstration_count: 1000 #100
    total_trial_count: 10000
    epsilon_min: 0.1
    epsilon_steps: 10 #1
    target_net_update_schedule: 10
    learning_rate: 1.0e-4
    lr_warmup_steps: 100
    model_publication_interval: 2000
    render_width: 1000
    batch_size: 32
    min_replay_buffer_size: 32
    max_replay_buffer_size: 10000
    flatten: False

connect_four_rainbow:
  <<: *default_params
  config:
    <<: *default_config
    num_input: 84
    num_action: 7
    player_count: 2
    agent_implementation: rainbowtorch
    environment_implementation: pet/connect_four_v3
    demonstration_count: 0 #100
    total_trial_count: 10000
    epsilon_steps: 5000
    model_publication_interval: 100
    aggregate_by_actor: True
    model_kwargs:
      v_min: -1.0
      v_max: 1.0

backgammon_rainbow:
  <<: *default_params
  config:
    <<: *default_config
    num_input: 198
    num_action: 1353
    player_count: 2
    agent_implementation: rainbowtorch
    environment_implementation: pet/backgammon_v3
    demonstration_count: 0 #100
    total_trial_count: 10
    model_publication_interval: 100

benchmark_lander:
  <<: *default_params
  config:
    <<: *default_config
    num_input: 8
    num_action: 4
    agent_implementation: rainbowtorch
    environment_implementation: gym/LunarLander-v2
    demonstration_count: 0
    total_trial_count: 250
    epsilon_min: 0.10
    epsilon_steps: 10000 #1
    target_net_update_schedule: 10
    learning_rate: 1.0e-4
    lr_warmup_steps: 100
    model_publication_interval: 250
    render_width: 1000
    batch_size: 32
    min_replay_buffer_size: 32
    model_kwargs:
      v_min: -200.0
      v_max: 200.0

benchmark_lander_hill:
  <<: *default_params
  config:
    <<: *default_config
    num_input: 8
    num_action: 4
    agent_implementation: rainbowtorch
    environment_implementation: gym/LunarLander-v2
    demonstration_count: 1000
    total_trial_count: 1000
    epsilon_min: 0.1
    epsilon_steps: 1000 #1
    target_net_update_schedule: 10
    learning_rate: 1.0e-4
    lr_warmup_steps: 100
    model_publication_interval: 250
    render_width: 1000
    batch_size: 256
    min_replay_buffer_size: 256
    model_kwargs:
      v_min: -200.0
      v_max: 200.0

lander_continuous_td3:
  <<: *default_params
  config:
    <<: *default_config
    num_input: 8
    num_action: 2
    agent_implementation: td3
    environment_implementation: gym/LunarLanderContinuous-v2
    demonstration_count: 0
    total_trial_count: 1000
    epsilon_min: 0.0
    epsilon_steps: 1
    target_net_update_schedule: 2
    learning_rate: 1.0e-4
    lr_warmup_steps: 100
    model_publication_interval: 250
    render_width: 1000
    batch_size: 256
    min_replay_buffer_size: 256
    max_replay_buffer_size: 100000
    max_parallel_trials: 8
    replay_buffer_config:
      observation_dtype: float32
      action_dtype: float32
    model_kwargs:
      start_timesteps: 25000
      high_action: [1, 1]
      low_action: [-1, -1]
      expl_noise: 0.1

lander_continuous_ddpg:
  <<: *default_params
  config:
    <<: *default_config
    num_input: 8
    num_action: 2
    agent_implementation: ddpg
    environment_implementation: gym/LunarLanderContinuous-v2
    demonstration_count: 0
    total_trial_count: 1000
    epsilon_min: 0.1
    epsilon_steps: 30000
    target_net_update_schedule: 2
    learning_rate: 1.0e-4
    lr_warmup_steps: 100
    model_publication_interval: 250
    render_width: 1000
    batch_size: 64
    min_replay_buffer_size: 64
    max_replay_buffer_size: 100000
    max_parallel_trials: 8
    replay_buffer_config:
      observation_dtype: float32
      action_dtype: float32
    model_kwargs:
      start_timesteps: 25000
      high_action: [1, 1]
      low_action: [-1, -1]
      expl_noise: 0.1

walker_td3:
  <<: *default_params
  config:
    <<: *default_config
    num_input: 8
    num_action: 2
    agent_implementation: td3
    environment_implementation: gym/BipedalWalker-v3
    demonstration_count: 0
    total_trial_count: 1000
    epsilon_min: 0.0
    epsilon_steps: 1
    target_net_update_schedule: 2
    learning_rate: 1.0e-4
    lr_warmup_steps: 100
    model_publication_interval: 250
    render_width: 1000
    batch_size: 256
    min_replay_buffer_size: 256
    max_replay_buffer_size: 100000
    max_parallel_trials: 8
    replay_buffer_config:
      observation_dtype: float32
      action_dtype: float32
    model_kwargs:
      start_timesteps: 25000
      high_action: [1, 1, 1, 1]
      low_action: [-1, -1, -1, -1]
      expl_noise: 0.1

walker_ddpg:
  <<: *default_params
  config:
    <<: *default_config
    num_input: 24
    num_action: 4
    agent_implementation: ddpg
    environment_implementation: gym/BipedalWalker-v3
    demonstration_count: 0
    total_trial_count: 1000
    epsilon_min: 0.1
    epsilon_steps: 50000
    target_net_update_schedule: 2
    learning_rate: 1.0e-4
    model_publication_interval: 250
    render_width: 1000
    batch_size: 64
    min_replay_buffer_size: 64
    max_replay_buffer_size: 100000
    replay_buffer_config:
      observation_dtype: float32
      action_dtype: float32
    model_kwargs:
      start_timesteps: 25000
      high_action: [1, 1, 1, 1]
      low_action: [-1, -1, -1, -1]
      expl_noise: 0.1


pendulum_td3:
  <<: *default_params
  config:
    <<: *default_config
    num_input: 3
    num_action: 1
    agent_implementation: td3
    environment_implementation: gym/Pendulum-v0
    demonstration_count: 0 #100
    total_trial_count: 250
    epsilon_min: 0.0
    epsilon_steps: 1
    target_net_update_schedule: 2
    #learning_rate: 3.0e-4
    learning_rate: 1.0e-4
    lr_warmup_steps: 100
    model_publication_interval: 1000
    render_width: 1000
    batch_size: 64
    min_replay_buffer_size: 64
    replay_buffer_config:
      observation_dtype: float32
      action_dtype: float32
    model_kwargs:
      start_timesteps: 5000
      high_action: [2]
      low_action: [-2]
      expl_noise: 0.1

pendulum_ddpg:
  <<: *default_params
  config:
    <<: *default_config
    num_input: 3
    num_action: 1
    agent_implementation: ddpg
    environment_implementation: gym/Pendulum-v0
    demonstration_count: 0 #100
    total_trial_count: 250
    epsilon_min: 0.0
    epsilon_steps: 1
    target_net_update_schedule: 2
    #learning_rate: 3.0e-4
    learning_rate: 1.0e-4
    lr_warmup_steps: 100
    model_publication_interval: 1000
    render_width: 1000
    batch_size: 64
    min_replay_buffer_size: 64
    max_replay_buffer_size: 10000
    replay_buffer_config:
      observation_dtype: float32
      action_dtype: float32
    model_kwargs:
      start_timesteps: 5000
      high_action: [2]
      low_action: [-2]
      expl_noise: 0.1

benchmark_breakout:
  <<: *default_params
  config:
    <<: *default_config
    num_input: 400
    num_action: 6
    agent_implementation: rainbowtorch
    environment_implementation: min/breakout
    demonstration_count: 0
    total_trial_count: 1000
    model_publication_interval: 100
    model_kwargs:
      v_min: -1.0
      v_max: 1.0
    batch_size: 32
    min_replay_buffer_size: 32

benchmark_connect_four:
  <<: *default_params
  config:
    <<: *default_config
    num_input: 84
    num_action: 7
    player_count: 2
    agent_implementation: rainbowtorch
    environment_implementation: pet/connect_four_v3
    demonstration_count: 0 #100
    total_trial_count: 10
    epsilon_steps: 5000
    model_publication_interval: 100
    aggregate_by_actor: True
    model_kwargs:
      v_min: -1.0
      v_max: 1.0
    batch_size: 32
    min_replay_buffer_size: 32

simple_a2c_cartpole: &default_simple_a2c_params
  implementation: simple_a2c_training
  config: &default_simple_a2c_config
    class_name: data_pb2.SimpleA2CTrainingRunConfig
    environment:
      implementation: gym/CartPole-v0
      config:
        seed: 12
    actor:
      num_input: 4
      num_action: 2
    training:
      epoch_count: 100
      epoch_trial_count: 15
      max_parallel_trials: 8
      discount_factor: 0.95
      entropy_coef: 0.01
      value_loss_coef: 0.5
      action_loss_coef: 1.0
      learning_rate: 0.01
    actor_network:
      hidden_size: 64
    critic_network:
      hidden_size: 64


# procgen environments follow
procgen_bigfish:
  <<: *default_params
  config:
    <<: *default_config
    num_input: 7056
    num_action: 15
    agent_implementation: atari_cnn
    environment_implementation: procgen/bigfish
    demonstration_count: 100
    total_trial_count: 10000
    model_publication_interval: 1000
    model_archive_interval_multiplier: 10
    max_parallel_trials: 2
    render_width: 64
    flatten: false
    batch_size: 32
    min_replay_buffer_size: 10000
    max_replay_buffer_size: 350000
    target_net_update_schedule: 100
    epsilon_steps: 100000
    learning_rate: 1.0e-4
    lr_warmup_steps: 10000
    framestack: 4
    model_kwargs:
      target_net_soft_update: False
      screensize: 64
    replay_buffer_config:
      observation_dtype: uint8
      action_dtype: int8
lander_muzero:
  implementation: muzero_mlp_training
  config:
    class_name: data_pb2.MuZeroTrainingRunConfig
    environment:
      env_type: "gym"
      env_name: "LunarLander-v2"
      seed: 12
    actor:
      num_input: 8
      num_action: 4
    training:
      model_publication_interval: 200
      trial_count: 2000
      max_parallel_trials: 4
      discount_rate: 0.99
      batch_size: 256
      representation_dim: 32
      projector_dim: 8
      hidden_dim: 256
      projector_hidden_dim: 64
      hidden_layers: 2
      projector_hidden_layers: 2
      min_replay_buffer_size: 5000
      max_replay_buffer_size: 10000
      exploration_alpha: 0.3
      log_interval: 100
      weight_decay: 0.01
      learning_rate: 0.0001
      min_learning_rate: 0.00001
      lr_warmup_steps: 200
      lr_decay_steps: 200000
      exploration_epsilon: 1.0
      epsilon_min: 0.1
      epsilon_decay_steps: 100000
      mcts_temperature: 1.0
      min_temperature: 1.0
      temperature_decay_steps: 0
      max_norm: 1.0
      rbins: 32
      vbins: 32
      rollout_length: 4
      mcts_depth: 8
      mcts_samples: 32
      target_label_smoothing_factor: 0.01
      target_label_smoothing_factor_steps: 0
      s_weight: 0.001
      v_weight: 0.1


cartpole_muzero: &default_muzero
  implementation: muzero_mlp_training
  config: &default_muzero_config
    class_name: data_pb2.MuZeroTrainingRunConfig
    environment:
      env_type: "gym"
      env_name: "CartPole-v0"
      seed: 12
    actor:
      num_input: 4
      num_action: 2
    training: &default_muzero_training_config
      model_publication_interval: 2000
      trial_count: 10000
      max_parallel_trials: 2
      discount_rate: 0.99
      batch_size: 128
      representation_dim: 16
      projector_dim: 16
      hidden_dim: 64
      hidden_layers: 2
      projector_hidden_dim: 32
      projector_hidden_layers: 1
      min_replay_buffer_size: 1000
      max_replay_buffer_size: 1000000
      log_interval: 100
      weight_decay: 0.01
      learning_rate: 0.001
      min_learning_rate: 0.0001
      lr_warmup_steps: 100
      lr_decay_steps: 100000
      exploration_alpha: 0.1
      exploration_epsilon: 0.25
      epsilon_min: 0.25
      epsilon_decay_steps: 1
      mcts_temperature: 1.0
      min_temperature: 0.25
      temperature_decay_steps: 1000
      max_norm: 100.0
      rbins: 4
      rmin: -1.0
      rmax: 1.0
      vbins: 16
      vmin: -0.1 # avoid proto default value???
      vmax: 200.0
      rollout_length: 4
      mcts_depth: 4
      mcts_samples: 16
      target_label_smoothing_factor: 0.01
      target_label_smoothing_factor_steps: 1
      s_weight: 1.0
      v_weight: 1.0
      ucb_c1: 1.25
      ucb_c2: 15000.0
      #train_device: "cuda:0"
      #actor_device: "cuda:1"

lander_muzero:
  <<: *default_muzero
  config:
    <<: *default_muzero_config
    environment:
      env_type: "gym"
      env_name: "LunarLander-v2"
      seed: 12
    actor:
      num_input: 8
      num_action: 4
    training:
      <<: *default_muzero_training_config
      rbins: 32
      rmin: -100.0
      rmax: 100.0
      vbins: 64
      vmin: -500.0 # avoid proto default value???
      vmax: 500.0
      batch_size: 128
      min_replay_buffer_size: 200
      demonstration_trials: 1000
      max_parallel_trials: 1
      mcts_depth: 4
      mcts_samples: 16