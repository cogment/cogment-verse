
environment_specs:
  cartpole_specs: &cartpole_specs
    implementation: gym/CartPole-v0
    num_players: 1
    num_input: 4
    num_action: 2
  pendulum_specs: &pendulum_specs
    implementation: gym/Pendulum-v0
    num_players: 1
    num_input: 3
    num_action: 1
  mountain_car_specs: &mountain_car_specs
    implementation: gym/MountainCar-v0
    num_players: 1
    num_input: 2
    num_action: 3
  lander_specs: &lander_specs
    implementation: gym/LunarLander-v2
    num_players: 1
    num_input: 8
    num_action: 4
  lander_continuous_specs: &lander_continuous_specs
    implementation: gym/LunarLanderContinuous-v2
    num_players: 1
    num_input: 8
    num_action: 2
  bipedal_walker_specs: &bipedal_walker_specs
    implementation: gym/BipedalWalker-v3
    num_players: 1
    num_input: 24
    num_action: 4
  minatar_breakout_specs: &minatar_breakout_specs
    implementation: minatar/breakout
    num_players: 1
    num_input: 500
    num_action: 6
  atari_pitfall_specs: &atari_pitfall_specs
    implementation: atari/Pitfall
    num_players: 1
    num_input: 7056
    num_action: 18
  atari_breakout_specs: &atari_breakout_specs
    implementation: atari/Breakout
    num_players: 1
    num_input: 7056
    num_action: 4
  tetris_specs: &tetris_specs
    implementation: tetris/TetrisA-v0
    num_players: 1
    num_input: 7056
    num_action: 12
  procgen_bigfish_specs: &procgen_bigfish_specs
    implementation: procgen/bigfish
    num_players: 1
    num_input: 7056
    num_action: 15
  pettingzoo_connect_four_specs: &pettingzoo_connect_four_specs
    implementation: pettingzoo/connect_four_v3
    num_players: 2
    num_input: 84
    num_action: 7
  pettingzoo_backgammon_specs: &pettingzoo_backgammon_specs
    implementation: pettingzoo/backgammon_v3
    num_players: 2
    num_input: 198
    num_action: 1353

play:
  implementation: play
  config:
    class_name: data_pb2.PlayRunConfig
    # Set to true to have the ability to observe the run in the web client
    observer: false
    trial_count: 10
    environment:
      specs: *mountain_car_specs
    actors:
    # Configure the players (only the first ones are used, up to the number of required players)
    - name: agent_1
      actor_class: agent
      implementation: random
      # To use an already trained agent use the following.
      # Make sure the model is compatible with the implementation and the environment
      # implementation: rainbowtorch
      # agent_config:
      #  model_id: compassionate_aryabhata_model
      #  model_version: -1
    - name: agent_2
      actor_class: agent
      implementation: random

cartpole_rainbow: &default_params
  implementation: "cogment_verse_run_impl"
  config: &default_config
    class_name: data_pb2.RunConfig
    environment:
      specs: *cartpole_specs
      config: &default_env_config
        render_width: 256
        flatten: True
        framestack: 1
    epsilon_min: 0.1
    epsilon_steps: 100000
    target_net_update_schedule: 1000
    learning_rate: 1.0e-4
    lr_warmup_steps: 10000
    demonstration_count: 0
    total_trial_count: 10000
    model_publication_interval: 1000
    model_archive_interval: 4000 # Archive every 4000 training steps
    batch_size: 256
    min_replay_buffer_size: 1000
    max_parallel_trials: 4
    model_kwargs: {}
    max_replay_buffer_size: 100000
    aggregate_by_actor: False
    replay_buffer_config:
      observation_dtype: float32
      action_dtype: int8
    agent_implementation: rainbowtorch

cartpole_dqn:
  <<: *default_params
  config:
    <<: *default_config
    agent_implementation: dqn

cartpole_REINFORCE:
  implementation: "reinforce_training"
  config:
    <<: *default_config
    agent_implementation: reinforce
    min_replay_buffer_size: 100000
    max_parallel_trials: 1
    learning_rate: 3.0e-4
    discount_factor: 0.99

pendulum_td3:
  <<: *default_params
  config:
    <<: *default_config
    environment:
      specs: *pendulum_specs
      config: *default_env_config
    agent_implementation: td3
    epsilon_min: 0.0
    epsilon_steps: 1
    target_net_update_schedule: 2
    learning_rate: 3.0e-4
    lr_warmup_steps: 100
    demonstration_count: 0
    #total_trial_count: 1000
    total_trial_count: 1000
    model_publication_interval: 2500
    min_replay_buffer_size: 256
    batch_size: 256
    replay_buffer_config:
      observation_dtype: float32
      action_dtype: float32

minatar_breakout_rainbow:
  <<: *default_params
  config:
    <<: *default_config
    environment:
      specs: *minatar_breakout_specs
      config: *default_env_config
    agent_implementation: rainbowtorch

atari_pitfall_rainbow:
  <<: *default_params
  config:
    <<: *default_config
    environment:
      specs: *atari_pitfall_specs
      config: *default_env_config
    agent_implementation: rainbowtorch

atari_breakout_cnn:
  <<: *default_params
  config:
    <<: *default_config
    environment:
      specs: *atari_breakout_specs
      config:
        <<: *default_env_config
        flatten: false
        framestack: 4
    agent_implementation: atari_cnn
    demonstration_count: 0
    total_trial_count: 10000
    model_publication_interval: 1000
    model_archive_interval: 1000000
    max_parallel_trials: 2
    batch_size: 32
    min_replay_buffer_size: 1000
    max_replay_buffer_size: 10000
    target_net_update_schedule: 100
    epsilon_steps: 1000000
    learning_rate: 1.0e-4
    lr_warmup_steps: 10000
    model_kwargs:
      target_net_soft_update: False
    replay_buffer_config:
      observation_dtype: uint8
      action_dtype: int8

minatar_breakout_demo:
  <<: *default_params
  config:
    <<: *default_config
    environment:
      specs: *minatar_breakout_specs
      config: *default_env_config
    agent_implementation: rainbowtorch

atari_breakout_demo:
  <<: *default_params
  config:
    <<: *default_config
    environment:
      specs: *atari_breakout_specs
      config:
        <<: *default_env_config
        framestack: 4
    agent_implementation: rainbowtorch

atari_pitfall_demo:
  <<: *default_params
  config:
    <<: *default_config
    environment:
      specs: *atari_pitfall_specs
      config: *default_env_config
    agent_implementation: rainbowtorch
    demonstration_count: 10

lander_rainbow_demo: &lander_rainbow_demo
  <<: *default_params
  config: &lander_rainbow_config
    <<: *default_config
    environment:
      specs: *lander_specs
      config:
        <<: *default_env_config
        render_width: 1000
    agent_implementation: rainbowtorch
    demonstration_count: 20000 #100
    total_trial_count: 20000
    epsilon_min: 0.1
    epsilon_steps: 10 #1
    target_net_update_schedule: 10
    learning_rate: 1.0e-4
    lr_warmup_steps: 100
    model_publication_interval: 200
    batch_size: 32
    min_replay_buffer_size: 32

lander_rainbow_demo_gpu:
  <<: *lander_rainbow_demo
  config:
    <<: *lander_rainbow_config
    agent_implementation: rainbowtorch
    model_kwargs:
      v_min: -200.0
      v_max: 200.0

tetris_rainbow_demo:
  <<: *default_params
  config:
    <<: *default_config
    environment:
      specs: *tetris_specs
      config:
        <<: *default_env_config
        render_width: 1000
        flatten: False
    agent_implementation: atari_cnn
    demonstration_count: 1000 #100
    total_trial_count: 10000
    epsilon_min: 0.1
    epsilon_steps: 10 #1
    target_net_update_schedule: 10
    learning_rate: 1.0e-4
    lr_warmup_steps: 100
    model_publication_interval: 2000
    batch_size: 32
    min_replay_buffer_size: 32
    max_replay_buffer_size: 10000

connect_four_rainbow:
  <<: *default_params
  config:
    <<: *default_config
    environment:
      specs: *pettingzoo_connect_four_specs
      config: *default_env_config
    agent_implementation: rainbowtorch
    demonstration_count: 0 #100
    total_trial_count: 10000
    epsilon_steps: 5000
    model_publication_interval: 100
    aggregate_by_actor: True
    model_kwargs:
      v_min: -1.0
      v_max: 1.0

backgammon_rainbow:
  <<: *default_params
  config:
    <<: *default_config
    environment:
      specs:
        implementation: pet/backgammon_v3
        num_input: 198
        num_action: 1353
      config: *default_env_config
    demonstration_count: 0 #100
    total_trial_count: 10
    model_publication_interval: 100

benchmark_lander:
  <<: *default_params
  config:
    <<: *default_config
    environment:
      specs: *lander_specs
      config:
        <<: *default_env_config
        render_width: 1000
    agent_implementation: rainbowtorch
    demonstration_count: 0
    total_trial_count: 250
    epsilon_min: 0.10
    epsilon_steps: 10000 #1
    target_net_update_schedule: 10
    learning_rate: 1.0e-4
    lr_warmup_steps: 100
    model_publication_interval: 250
    batch_size: 32
    min_replay_buffer_size: 32
    model_kwargs:
      v_min: -200.0
      v_max: 200.0

benchmark_lander_hill:
  <<: *default_params
  config:
    <<: *default_config
    environment:
      specs: *lander_specs
      config:
        <<: *default_env_config
        render_width: 1000
    agent_implementation: rainbowtorch
    demonstration_count: 1000
    total_trial_count: 1000
    epsilon_min: 0.1
    epsilon_steps: 1000 #1
    target_net_update_schedule: 10
    learning_rate: 1.0e-4
    lr_warmup_steps: 100
    model_publication_interval: 250
    batch_size: 256
    min_replay_buffer_size: 256
    model_kwargs:
      v_min: -200.0
      v_max: 200.0

lander_continuous_td3:
  <<: *default_params
  config:
    <<: *default_config
    environment:
      specs: *lander_continuous_specs
      config:
        <<: *default_env_config
        render_width: 1000
    agent_implementation: td3
    demonstration_count: 0
    total_trial_count: 1000
    epsilon_min: 0.0
    epsilon_steps: 1
    target_net_update_schedule: 2
    learning_rate: 1.0e-4
    lr_warmup_steps: 100
    model_publication_interval: 250
    batch_size: 256
    min_replay_buffer_size: 256
    max_replay_buffer_size: 100000
    max_parallel_trials: 8
    replay_buffer_config:
      observation_dtype: float32
      action_dtype: float32
    model_kwargs:
      start_timesteps: 25000
      high_action: [1, 1]
      low_action: [-1, -1]
      expl_noise: 0.1

lander_continuous_ddpg:
  <<: *default_params
  config:
    <<: *default_config
    environment:
      specs: *lander_continuous_specs
      config:
        <<: *default_env_config
        render_width: 1000
    agent_implementation: ddpg
    demonstration_count: 0
    total_trial_count: 1000
    epsilon_min: 0.1
    epsilon_steps: 30000
    target_net_update_schedule: 2
    learning_rate: 1.0e-4
    lr_warmup_steps: 100
    model_publication_interval: 250
    batch_size: 64
    min_replay_buffer_size: 64
    max_replay_buffer_size: 100000
    max_parallel_trials: 8
    replay_buffer_config:
      observation_dtype: float32
      action_dtype: float32
    model_kwargs:
      start_timesteps: 25000
      high_action: [1, 1]
      low_action: [-1, -1]
      expl_noise: 0.1

walker_td3:
  <<: *default_params
  config:
    <<: *default_config
    environment:
      specs: *bipedal_walker_specs
      config:
        <<: *default_env_config
        render_width: 1000
    agent_implementation: td3
    demonstration_count: 0
    total_trial_count: 1000
    epsilon_min: 0.0
    epsilon_steps: 1
    target_net_update_schedule: 2
    learning_rate: 1.0e-4
    lr_warmup_steps: 100
    model_publication_interval: 250
    batch_size: 256
    min_replay_buffer_size: 256
    max_replay_buffer_size: 100000
    max_parallel_trials: 8
    replay_buffer_config:
      observation_dtype: float32
      action_dtype: float32
    model_kwargs:
      start_timesteps: 25000
      high_action: [1, 1, 1, 1]
      low_action: [-1, -1, -1, -1]
      expl_noise: 0.1

walker_ddpg:
  <<: *default_params
  config:
    <<: *default_config
    environment:
      specs: *bipedal_walker_specs
      config:
        <<: *default_env_config
        render_width: 1000
    agent_implementation: ddpg
    demonstration_count: 0
    total_trial_count: 1000
    epsilon_min: 0.1
    epsilon_steps: 50000
    target_net_update_schedule: 2
    learning_rate: 1.0e-4
    model_publication_interval: 250
    batch_size: 64
    min_replay_buffer_size: 64
    max_replay_buffer_size: 100000
    replay_buffer_config:
      observation_dtype: float32
      action_dtype: float32
    model_kwargs:
      start_timesteps: 25000
      high_action: [1, 1, 1, 1]
      low_action: [-1, -1, -1, -1]
      expl_noise: 0.1


pendulum_td3:
  <<: *default_params
  config:
    <<: *default_config
    environment:
      specs:
        implementation: gym/Pendulum-v0
        num_input: 3
        num_action: 1
      config:
        <<: *default_env_config
        render_width: 1000
    agent_implementation: td3
    demonstration_count: 0 #100
    total_trial_count: 250
    epsilon_min: 0.0
    epsilon_steps: 1
    target_net_update_schedule: 2
    #learning_rate: 3.0e-4
    learning_rate: 1.0e-4
    lr_warmup_steps: 100
    model_publication_interval: 1000
    batch_size: 64
    min_replay_buffer_size: 64
    replay_buffer_config:
      observation_dtype: float32
      action_dtype: float32
    model_kwargs:
      start_timesteps: 5000
      high_action: [2]
      low_action: [-2]
      expl_noise: 0.1

pendulum_ddpg:
  <<: *default_params
  config:
    <<: *default_config
    environment:
      specs:
        implementation: gym/Pendulum-v0
        num_input: 3
        num_action: 1
      config:
        <<: *default_env_config
        render_width: 1000
    agent_implementation: ddpg
    demonstration_count: 0 #100
    total_trial_count: 250
    epsilon_min: 0.0
    epsilon_steps: 1
    target_net_update_schedule: 2
    #learning_rate: 3.0e-4
    learning_rate: 1.0e-4
    lr_warmup_steps: 100
    model_publication_interval: 1000
    batch_size: 64
    min_replay_buffer_size: 64
    max_replay_buffer_size: 10000
    replay_buffer_config:
      observation_dtype: float32
      action_dtype: float32
    model_kwargs:
      start_timesteps: 5000
      high_action: [2]
      low_action: [-2]
      expl_noise: 0.1

benchmark_breakout:
  <<: *default_params
  config:
    <<: *default_config
    environment:
      specs: *minatar_breakout_specs
      config: *default_env_config
    agent_implementation: rainbowtorch
    demonstration_count: 0
    total_trial_count: 1000
    model_publication_interval: 100
    model_kwargs:
      v_min: -1.0
      v_max: 1.0
    batch_size: 32
    min_replay_buffer_size: 32

benchmark_connect_four:
  <<: *default_params
  config:
    <<: *default_config
    environment:
      specs: *pettingzoo_connect_four_specs
      config: *default_env_config
    agent_implementation: rainbowtorch
    environment_implementation:
    demonstration_count: 0 #100
    total_trial_count: 10
    epsilon_steps: 5000
    model_publication_interval: 100
    aggregate_by_actor: True
    model_kwargs:
      v_min: -1.0
      v_max: 1.0
    batch_size: 32
    min_replay_buffer_size: 32

simple_a2c_cartpole: &default_simple_a2c_params
  implementation: simple_a2c_training
  config: &default_simple_a2c_config
    class_name: data_pb2.SimpleA2CTrainingRunConfig
    environment:
      specs: *cartpole_specs
      config:
        <<: *default_env_config
        seed: 12
    training:
      epoch_count: 100
      epoch_trial_count: 15
      max_parallel_trials: 8
      discount_factor: 0.95
      entropy_coef: 0.01
      value_loss_coef: 0.5
      action_loss_coef: 1.0
      learning_rate: 0.01
    actor_network:
      hidden_size: 64
    critic_network:
      hidden_size: 64

selfplay_td3_driving: &default_selfplay_td3_params
  implementation: selfplay_td3_training
  config: &default_elfplay_td3_config
    class_name: data_pb2.SelfPlayTD3TrainingRunConfig
    environment:
      specs:
        implementation: driving/SimpleDriving-v0
        num_players: 2
      config:
        seed: 12
        render_width: 1000
        framestack: 4
    actor:
      implementation: selfplay_td3
      config:
        num_input: 7
        num_input_2: 2
        num_action: 2
        action_scale: [ 0.5,0.6 ]
        action_bias: [ 1.0,0.0 ]
        max_action: 1
        alice_grid_shape: [75, 75, 2]
        bob_grid_shape: [75, 75, 3]
        actor_network:
          hidden_size: 64
        critic_network:
          hidden_size: 64
        model_kwargs:
          start_timesteps: 5000
          high_action: [ 0, -1 ]
          low_action: [ 1, 1 ]
          expl_noise: 0.1
    rollout:
      epoch_count: 1000
      epoch_train_trial_count: 2
      epoch_test_trial_count: 2
      max_parallel_trials: 1
      model_publication_interval: 10
      number_turns_per_trial: 5
      test_freq: 1
    training:
      batch_size: 64
      discount_factor: 0.99
      tau: 0.005
      policy_noise: 0.1
      noise_clip: 0.2
      learning_rate: 3e-4
      policy_freq: 2
      SIGMA: 0.2
      num_training_steps: 2
      beta: 0.05
      alice_reward: 5.0
      bob_reward: 5.0
      alice_penalty: -5.0
      bob_penalty: -2.0
    replaybuffer:
      min_replay_buffer_size: 256
      max_replay_buffer_size: 100000
      replay_buffer_config:
        observation_dtype: float32
        action_dtype: float32

# procgen environments follow
procgen_bigfish:
  <<: *default_params
  config:
    <<: *default_config
    environment:
      specs: *procgen_bigfish_specs
      config:
        <<: *default_env_config
        render_width: 64
        flatten: false
        framestack: 4
    agent_implementation: atari_cnn
    demonstration_count: 100
    total_trial_count: 10000
    model_publication_interval: 1000
    max_parallel_trials: 2
    batch_size: 32
    min_replay_buffer_size: 10000
    max_replay_buffer_size: 350000
    target_net_update_schedule: 100
    epsilon_steps: 100000
    learning_rate: 1.0e-4
    lr_warmup_steps: 10000
    model_kwargs:
      target_net_soft_update: False
      screensize: 64
    replay_buffer_config:
      observation_dtype: uint8
      action_dtype: int8

cartpole_muzero: &default_muzero
  implementation: muzero_mlp_training
  config: &default_muzero_config
    class_name: data_pb2.MuZeroRunConfig
    environment:
      specs: *cartpole_specs
      config: *default_env_config
    reward_distribution:
      min_value: -1.0
      max_value: 1.0
      num_bins: 4
    value_distribution:
      min_value: -200.0
      max_value: 200.0
      num_bins: 32

lander_muzero:
  implementation: muzero_mlp_training
  config: &lander_muzero_config
    <<: *default_muzero_config
    environment:
      specs: *lander_specs
      config: *default_env_config
    reward_distribution:
      min_value: -100.0
      max_value: 100.0
      num_bins: 32
    value_distribution:
      min_value: -1000.0
      max_value: 1000.0
      num_bins: 128

lander_muzero_gpu:
  implementation: muzero_mlp_training
  config:
    <<: *lander_muzero_config
    train_device: "cuda:1"
    reanalyze_device: "cuda:1"
    actor_device: "cuda:0"

simple_bc_lander: &simple_bc_experiment
  implementation: simple_bc_training
  config: &simple_bc_config
    class_name: data_pb2.SimpleBCTrainingRunConfig
    environment:
      specs: *lander_specs
      config:
        seed: 12
    training:
      trial_count: 1000
      max_parallel_trials: 1
      learning_rate: 0.001
      batch_size: 32
    policy_network:
      hidden_size: 64

simple_bc_cartpole:
  <<: *simple_bc_experiment
  implementation: simple_bc_training
  config:
    <<: *simple_bc_config
    environment:
      specs: *cartpole_specs
      config:
        seed: 12

simple_bc_mountaincar:
  <<: *simple_bc_experiment
  implementation: simple_bc_training
  config:
    <<: *simple_bc_config
    environment:
      specs: *mountain_car_specs
      config:
        seed: 12

simple_sb3_cartpole:
  implementation: play
  config:
    class_name: data_pb2.PlayRunConfig
    observer: false
    trial_count: 10
    environment:
      specs: *cartpole_specs
    actors:
    - name: agent_1
      actor_class: agent
      implementation: simple_sb3
      agent_config:
        hf_hub_model:
          repo_id: "sb3/demo-hf-CartPole-v0"
          filename: "ppo-CartPole-v0.zip"

simple_sb3_lander:
  implementation: play
  config:
    class_name: data_pb2.PlayRunConfig
    observer: false
    trial_count: 10
    environment:
      specs: *lander_specs
    actors:
    - name: agent_1
      actor_class: agent
      implementation: simple_sb3
      agent_config:
        hf_hub_model:
          repo_id: "ThomasSimonini/ppo-LunarLander-v2"
          filename: "ppo-LunarLander-v2.zip"
