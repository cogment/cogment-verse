
cartpole_rainbow: &default_params
  implementation: "cogment_verse_run_impl"
  config: &default_config
    class_name: data_pb2.RunConfig
    player_count: 1
    epsilon_min: 0.1
    epsilon_steps: 100000
    target_net_update_schedule: 1000
    learning_rate: 1.0e-4
    lr_warmup_steps: 10000
    demonstration_count: 0
    total_trial_count: 10000
    model_publication_interval: 1000
    model_archive_interval_multiplier: 4 # Archive every fourth published model
    render_width: 256
    batch_size: 256
    min_replay_buffer_size: 1000
    max_parallel_trials: 4
    model_kwargs: {}
    max_replay_buffer_size: 100000
    flatten: True
    aggregate_by_actor: False
    framestack: 1
    replay_buffer_config:
      observation_dtype: float32
      action_dtype: int8
    num_input: 4
    num_action: 2
    agent_implementation: rainbowtorch
    # environment_implementation: gym/CartPole-v0environment_implementation

cartpole_dqn:
  <<: *default_params
  config:
    <<: *default_config
    num_input: 4
    num_action: 2
    agent_implementation: dqn
    environment_implementation: gym/CartPole-v0

cartpole_REINFORCE:
  implementation: "reinforce_training"
  config:
    <<: *default_config
    agent_implementation: reinforce
    min_replay_buffer_size: 100000
    max_parallel_trials: 1
    learning_rate: 3.0e-4
    discount_factor: 0.99

pendulum_td3:
  <<: *default_params
  config:
    <<: *default_config
    num_input: 3
    num_action: 1
    agent_implementation: td3
    environment_implementation: gym/Pendulum-v0
    epsilon_min: 0.0
    epsilon_steps: 1
    target_net_update_schedule: 2
    learning_rate: 3.0e-4
    lr_warmup_steps: 100
    demonstration_count: 0
    #total_trial_count: 1000
    total_trial_count: 1000
    model_publication_interval: 2500
    min_replay_buffer_size: 256
    batch_size: 256
    replay_buffer_config:
      observation_dtype: float32
      action_dtype: float32

minatar_breakout_rainbow:
  <<: *default_params
  config:
    <<: *default_config
    num_input: 400
    num_action: 6
    agent_implementation: rainbowtorch
    environment_implementation: min/breakout

atari_pitfall_rainbow:
  <<: *default_params
  config:
    <<: *default_config
    num_input: 7056
    num_action: 18
    agent_implementation: rainbowtorch
    environment_implementation: ata/Pitfall

atari_breakout_cnn:
  <<: *default_params
  config:
    <<: *default_config
    num_input: 7056
    num_action: 4
    agent_implementation: atari_cnn
    environment_implementation: ata/Breakout
    demonstration_count: 0
    total_trial_count: 10000
    model_publication_interval: 1000
    model_archive_interval_multiplier: 100
    max_parallel_trials: 2
    render_width: 256
    flatten: false
    batch_size: 1024
    min_replay_buffer_size: 10000
    max_replay_buffer_size: 350000
    target_net_update_schedule: 100
    epsilon_steps: 100000
    learning_rate: 1.0e-4
    lr_warmup_steps: 10000
    framestack: 4
    model_kwargs:
      target_net_soft_update: False
    replay_buffer_config:
      observation_dtype: uint8
      action_dtype: int8

minatar_breakout_demo:
  <<: *default_params
  config:
    <<: *default_config
    num_input: 400
    num_action: 6
    agent_implementation: rainbowtorch
    environment_implementation: min/breakout
    demonstration_count: 0

atari_breakout_demo:
  <<: *default_params
  config:
    <<: *default_config
    num_input: 7056
    num_action: 4
    agent_implementation: rainbowtorch
    environment_implementation: ata/Breakout
    demonstration_count: 0
    framestack: 4

atari_pitfall_demo:
  <<: *default_params
  config:
    <<: *default_config
    num_input: 7056
    num_action: 18
    agent_implementation: rainbowtorch
    environment_implementation: ata/Pitfall
    demonstration_count: 10

lander_rainbow_demo: &lander_rainbow_demo
  <<: *default_params
  config: &lander_rainbow_config
    <<: *default_config
    num_input: 8
    num_action: 4
    agent_implementation: rainbowtorch
    environment_implementation: gym/LunarLander-v2
    demonstration_count: 20000 #100
    total_trial_count: 20000
    epsilon_min: 0.1
    epsilon_steps: 10 #1
    target_net_update_schedule: 10
    learning_rate: 1.0e-4
    lr_warmup_steps: 100
    model_publication_interval: 200
    render_width: 1000
    batch_size: 32
    min_replay_buffer_size: 32

lander_rainbow_demo_gpu:
  <<: *lander_rainbow_demo
  config:
    <<: *lander_rainbow_config
    agent_implementation: rainbowtorch
    model_kwargs:
      v_min: -200.0
      v_max: 200.0

tetris_rainbow_demo:
  <<: *default_params
  config:
    <<: *default_config
    num_input: 7056
    num_action: 12
    agent_implementation: atari_cnn
    environment_implementation: tet/TetrisA-v0
    demonstration_count: 1000 #100
    total_trial_count: 10000
    epsilon_min: 0.1
    epsilon_steps: 10 #1
    target_net_update_schedule: 10
    learning_rate: 1.0e-4
    lr_warmup_steps: 100
    model_publication_interval: 2000
    render_width: 1000
    batch_size: 32
    min_replay_buffer_size: 32
    max_replay_buffer_size: 10000
    flatten: False

connect_four_rainbow:
  <<: *default_params
  config:
    <<: *default_config
    num_input: 84
    num_action: 7
    player_count: 2
    agent_implementation: rainbowtorch
    environment_implementation: pet/connect_four_v3
    demonstration_count: 0 #100
    total_trial_count: 10000
    epsilon_steps: 5000
    model_publication_interval: 100
    aggregate_by_actor: True
    model_kwargs:
      v_min: -1.0
      v_max: 1.0

backgammon_rainbow:
  <<: *default_params
  config:
    <<: *default_config
    num_input: 198
    num_action: 1353
    player_count: 2
    agent_implementation: rainbowtorch
    environment_implementation: pet/backgammon_v3
    demonstration_count: 0 #100
    total_trial_count: 10
    model_publication_interval: 100

benchmark_lander:
  <<: *default_params
  config:
    <<: *default_config
    num_input: 8
    num_action: 4
    agent_implementation: rainbowtorch
    environment_implementation: gym/LunarLander-v2
    demonstration_count: 0
    total_trial_count: 250
    epsilon_min: 0.10
    epsilon_steps: 10000 #1
    target_net_update_schedule: 10
    learning_rate: 1.0e-4
    lr_warmup_steps: 100
    model_publication_interval: 250
    render_width: 1000
    batch_size: 32
    min_replay_buffer_size: 32
    model_kwargs:
      v_min: -200.0
      v_max: 200.0

benchmark_lander_hill:
  <<: *default_params
  config:
    <<: *default_config
    num_input: 8
    num_action: 4
    agent_implementation: rainbowtorch
    environment_implementation: gym/LunarLander-v2
    demonstration_count: 1000
    total_trial_count: 1000
    epsilon_min: 0.1
    epsilon_steps: 1000 #1
    target_net_update_schedule: 10
    learning_rate: 1.0e-4
    lr_warmup_steps: 100
    model_publication_interval: 250
    render_width: 1000
    batch_size: 256
    min_replay_buffer_size: 256
    model_kwargs:
      v_min: -200.0
      v_max: 200.0

lander_continuous_td3:
  <<: *default_params
  config:
    <<: *default_config
    num_input: 8
    num_action: 2
    agent_implementation: td3
    environment_implementation: gym/LunarLanderContinuous-v2
    demonstration_count: 0
    total_trial_count: 1000
    epsilon_min: 0.0
    epsilon_steps: 1
    target_net_update_schedule: 2
    learning_rate: 1.0e-4
    lr_warmup_steps: 100
    model_publication_interval: 250
    render_width: 1000
    batch_size: 256
    min_replay_buffer_size: 256
    max_replay_buffer_size: 100000
    max_parallel_trials: 8
    replay_buffer_config:
      observation_dtype: float32
      action_dtype: float32
    model_kwargs:
      start_timesteps: 25000
      high_action: [1, 1]
      low_action: [-1, -1]
      expl_noise: 0.1

lander_continuous_ddpg:
  <<: *default_params
  config:
    <<: *default_config
    num_input: 8
    num_action: 2
    agent_implementation: ddpg
    environment_implementation: gym/LunarLanderContinuous-v2
    demonstration_count: 0
    total_trial_count: 1000
    epsilon_min: 0.1
    epsilon_steps: 30000
    target_net_update_schedule: 2
    learning_rate: 1.0e-4
    lr_warmup_steps: 100
    model_publication_interval: 250
    render_width: 1000
    batch_size: 64
    min_replay_buffer_size: 64
    max_replay_buffer_size: 100000
    max_parallel_trials: 8
    replay_buffer_config:
      observation_dtype: float32
      action_dtype: float32
    model_kwargs:
      start_timesteps: 25000
      high_action: [1, 1]
      low_action: [-1, -1]
      expl_noise: 0.1

walker_td3:
  <<: *default_params
  config:
    <<: *default_config
    num_input: 8
    num_action: 2
    agent_implementation: td3
    environment_implementation: gym/BipedalWalker-v3
    demonstration_count: 0
    total_trial_count: 1000
    epsilon_min: 0.0
    epsilon_steps: 1
    target_net_update_schedule: 2
    learning_rate: 1.0e-4
    lr_warmup_steps: 100
    model_publication_interval: 250
    render_width: 1000
    batch_size: 256
    min_replay_buffer_size: 256
    max_replay_buffer_size: 100000
    max_parallel_trials: 8
    replay_buffer_config:
      observation_dtype: float32
      action_dtype: float32
    model_kwargs:
      start_timesteps: 25000
      high_action: [1, 1, 1, 1]
      low_action: [-1, -1, -1, -1]
      expl_noise: 0.1

walker_ddpg:
  <<: *default_params
  config:
    <<: *default_config
    num_input: 24
    num_action: 4
    agent_implementation: ddpg
    environment_implementation: gym/BipedalWalker-v3
    demonstration_count: 0
    total_trial_count: 1000
    epsilon_min: 0.1
    epsilon_steps: 50000
    target_net_update_schedule: 2
    learning_rate: 1.0e-4
    model_publication_interval: 250
    render_width: 1000
    batch_size: 64
    min_replay_buffer_size: 64
    max_replay_buffer_size: 100000
    replay_buffer_config:
      observation_dtype: float32
      action_dtype: float32
    model_kwargs:
      start_timesteps: 25000
      high_action: [1, 1, 1, 1]
      low_action: [-1, -1, -1, -1]
      expl_noise: 0.1


pendulum_td3:
  <<: *default_params
  config:
    <<: *default_config
    num_input: 3
    num_action: 1
    agent_implementation: td3
    environment_implementation: gym/Pendulum-v0
    demonstration_count: 0 #100
    total_trial_count: 250
    epsilon_min: 0.0
    epsilon_steps: 1
    target_net_update_schedule: 2
    #learning_rate: 3.0e-4
    learning_rate: 1.0e-4
    lr_warmup_steps: 100
    model_publication_interval: 1000
    render_width: 1000
    batch_size: 64
    min_replay_buffer_size: 64
    replay_buffer_config:
      observation_dtype: float32
      action_dtype: float32
    model_kwargs:
      start_timesteps: 5000
      high_action: [2]
      low_action: [-2]
      expl_noise: 0.1

pendulum_ddpg:
  <<: *default_params
  config:
    <<: *default_config
    num_input: 3
    num_action: 1
    agent_implementation: ddpg
    environment_implementation: gym/Pendulum-v0
    demonstration_count: 0 #100
    total_trial_count: 250
    epsilon_min: 0.0
    epsilon_steps: 1
    target_net_update_schedule: 2
    #learning_rate: 3.0e-4
    learning_rate: 1.0e-4
    lr_warmup_steps: 100
    model_publication_interval: 1000
    render_width: 1000
    batch_size: 64
    min_replay_buffer_size: 64
    max_replay_buffer_size: 10000
    replay_buffer_config:
      observation_dtype: float32
      action_dtype: float32
    model_kwargs:
      start_timesteps: 5000
      high_action: [2]
      low_action: [-2]
      expl_noise: 0.1

benchmark_breakout:
  <<: *default_params
  config:
    <<: *default_config
    num_input: 400
    num_action: 6
    agent_implementation: rainbowtorch
    environment_implementation: min/breakout
    demonstration_count: 0
    total_trial_count: 1000
    model_publication_interval: 100
    model_kwargs:
      v_min: -1.0
      v_max: 1.0
    batch_size: 32
    min_replay_buffer_size: 32

benchmark_connect_four:
  <<: *default_params
  config:
    <<: *default_config
    num_input: 84
    num_action: 7
    player_count: 2
    agent_implementation: rainbowtorch
    environment_implementation: pet/connect_four_v3
    demonstration_count: 0 #100
    total_trial_count: 10
    epsilon_steps: 5000
    model_publication_interval: 100
    aggregate_by_actor: True
    model_kwargs:
      v_min: -1.0
      v_max: 1.0
    batch_size: 32
    min_replay_buffer_size: 32

simple_a2c_cartpole: &default_simple_a2c_params
  implementation: simple_a2c_training
  config: &default_simple_a2c_config
    class_name: data_pb2.SimpleA2CTrainingRunConfig
    environment:
      implementation: gym/CartPole-v0
      config:
        seed: 12
    actor:
      num_input: 4
      num_action: 2
    training:
      epoch_count: 100
      epoch_trial_count: 15
      max_parallel_trials: 8
      discount_factor: 0.95
      entropy_coef: 0.01
      value_loss_coef: 0.5
      action_loss_coef: 1.0
      learning_rate: 0.01
    actor_network:
      hidden_size: 64
    critic_network:
      hidden_size: 64


# procgen environments follow
procgen_bigfish:
  <<: *default_params
  config:
    <<: *default_config
    num_input: 7056
    num_action: 15
    agent_implementation: atari_cnn
    environment_implementation: procgen/bigfish
    demonstration_count: 100
    total_trial_count: 10000
    model_publication_interval: 1000
    model_archive_interval_multiplier: 10
    max_parallel_trials: 2
    render_width: 64
    flatten: false
    batch_size: 32
    min_replay_buffer_size: 10000
    max_replay_buffer_size: 350000
    target_net_update_schedule: 100
    epsilon_steps: 100000
    learning_rate: 1.0e-4
    lr_warmup_steps: 10000
    framestack: 4
    model_kwargs:
      target_net_soft_update: False
      screensize: 64
    replay_buffer_config:
      observation_dtype: uint8
      action_dtype: int8

driving_td3:
  <<: *default_params
  config:
    <<: *default_config
    num_input: 7
    num_input_2: [75,75]
    num_action: 2
    agent_implementation: td3
    environment_implementation: driving/SimpleDriving-v0
    demonstration_count: 0 #100
    total_trial_count: 250
    epsilon_min: 0.0
    epsilon_steps: 1
    target_net_update_schedule: 2
    #learning_rate: 3.0e-4
    learning_rate: 1.0e-4
    lr_warmup_steps: 100
    model_publication_interval: 1000
    render_width: 1000
    batch_size: 64
    min_replay_buffer_size: 64
    replay_buffer_config:
      observation_dtype: float32
      action_dtype: float32
    model_kwargs:
      start_timesteps: 5000
      high_action: [0, -1]
      low_action: [1, 1]
      expl_noise: 0.1