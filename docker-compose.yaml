version: "3.7"

services:
  # Base images, they are defined here to be built when running `docker compose build`
  base_python:
    build: base_python
    image: cogment/cogment-verse-python-sdk:${IMAGE_TAG:-local}
    command: echo "Not a service, exiting"

  orchestrator:
    image: cogment/orchestrator:v2.0.0
    environment:
      - COGMENT_VERSE_TRIAL_DATASTORE_ENDPOINT
      - COGMENT_LIFECYCLE_PORT=${COGMENT_VERSE_ORCHESTRATOR_PORT}
      - COGMENT_ACTOR_PORT=${COGMENT_VERSE_ORCHESTRATOR_PORT}
      - COGMENT_PRE_TRIAL_HOOKS=grpc://${COGMENT_VERSE_PRETRIAL_HOOK_ENDPOINT}
    depends_on:
      - trial_datastore

  trial_datastore:
    image: cogment/trial-datastore:v0.2.0
    restart: on-failure
    environment:
      - COGMENT_TRIAL_DATASTORE_PORT=${COGMENT_VERSE_TRIAL_DATASTORE_PORT}
      # Maximum size (in bytes) of the trial datastore memory storage
      # Set to 1GB = 1073741824B
      - COGMENT_TRIAL_DATASTORE_MEMORY_STORAGE_MAX_SAMPLE_SIZE=1073741824
      # Log level (trace/debug/info/warn/error)
      - COGMENT_TRIAL_DATASTORE_LOG_LEVEL=info

  model_registry:
    image: cogment/model-registry:v0.4.0
    restart: on-failure
    environment:
      - COGMENT_MODEL_REGISTRY_PORT=${COGMENT_VERSE_MODEL_REGISTRY_PORT}
      # Limit chunk size due to GRPC message size limit
      - COGMENT_MODEL_REGISTRY_SENT_MODEL_VERSION_DATA_CHUNK_SIZE=2097152
      # Set memory cache maximum size to 1GB
      - COGMENT_MODEL_REGISTRY_VERSION_CACHE_MAX_SIZE=1073741824
      # Set memory cache expiration to 1 hour, can be set to any string
      # that can be parsed by Go's time.ParseDuration
      - COGMENT_MODEL_REGISTRY_VERSION_CACHE_EXPIRATION=1h
      # Amount of model versions that are removed from the cache at once
      - COGMENT_MODEL_REGISTRY_VERSION_CACHE_TO_PRUNE_COUNT=50
    volumes:
      - ./data/model-registry:/data

  environment:
    image: cogment/cogment-verse-environment:${IMAGE_TAG:-local}
    build:
      context: ./environment
      args:
        BASE_PYTHON_IMAGE: cogment/cogment-verse-python-sdk:${IMAGE_TAG:-local}
    environment:
      - COGMENT_VERSE_ENVIRONMENT_PORT
      - COGMENT_VERSE_ENVIRONMENT_PROMETHEUS_PORT
    init: true # xvfb-run hang fix?
    tty: true

  torch_agents:
    &torch_agents
    build:
      context: torch_agents
      args:
        DEVICE: cpu
        BASE_PYTHON_IMAGE: cogment/cogment-verse-python-sdk:${IMAGE_TAG:-local}
    restart: on-failure
    environment:
      - PYTHONUNBUFFERED=1
      - COGMENT_VERSE_TORCH_AGENTS_PORT
      - COGMENT_VERSE_TORCH_AGENTS_PROMETHEUS_PORT
      - COGMENT_VERSE_MODEL_REGISTRY_ENDPOINT
      - COGMENT_VERSE_TRIAL_DATASTORE_ENDPOINT
      - COGMENT_VERSE_ENVIRONMENT_ENDPOINTS
      - COGMENT_VERSE_ORCHESTRATOR_ENDPOINT
      - COGMENT_VERSE_ACTOR_ENDPOINTS
      - MLFLOW_TRACKING_URI
      - MPLBACKEND=Agg
    depends_on:
      - orchestrator
      - environment
      - model_registry
      - trial_datastore
      - mlflow

  tf_agents:
    &tf_agents
    build:
      context: tf_agents
      args:
        BASE_PYTHON_IMAGE: cogment/cogment-verse-python-sdk:${IMAGE_TAG:-local}
    restart: on-failure
    environment:
      - PYTHONUNBUFFERED=1
      - COGMENT_VERSE_TF_AGENTS_PORT
      - COGMENT_VERSE_TF_AGENTS_PROMETHEUS_PORT
      - COGMENT_VERSE_MODEL_REGISTRY_ENDPOINT
      - COGMENT_VERSE_TRIAL_DATASTORE_ENDPOINT
      - COGMENT_VERSE_ENVIRONMENT_ENDPOINTS
      - COGMENT_VERSE_ORCHESTRATOR_ENDPOINT
      - COGMENT_VERSE_ACTOR_ENDPOINTS
      - MLFLOW_TRACKING_URI
      - MPLBACKEND=Agg
    depends_on:
      - orchestrator
      - environment
      - model_registry
      - trial_datastore
      - mlflow

  client:
    build: client
    command: --params_path=/cogment-verse/client/run_params.yaml
    environment:
      - COGMENT_VERSE_RUN_ENDPOINTS
    volumes:
      - ./run_params.yaml:/cogment-verse/client/run_params.yaml

  grafana:
    image: cogment/cogment-verse-grafana:${IMAGE_TAG:-local}
    build: ./grafana
    restart: on-failure
    user: 0:0
    environment:
      - COGMENT_VERSE_GRAFANA_PORT
      - COGMENT_VERSE_PROMETHEUS_URL
    ports:
      - ${COGMENT_VERSE_GRAFANA_EXPOSED_PORT}:${COGMENT_VERSE_GRAFANA_PORT}/tcp
    volumes:
      - ./data/grafana:/data
    depends_on:
      - prometheus

  prometheus:
    image: cogment/cogment-verse-prometheus:${IMAGE_TAG:-local}
    build: ./prometheus
    restart: on-failure
    user: 0:0
    environment:
      - COGMENT_VERSE_PROMETHEUS_PORT
      - COGMENT_VERSE_PROMETHEUS_TARGETS
    volumes:
      - ./data/prometheus:/prometheus

  mlflow:
    build: ./mlflow
    restart: on-failure
    environment:
      - COGMENT_VERSE_MLFLOW_PORT
    ports:
      - ${COGMENT_VERSE_MLFLOW_EXPOSED_PORT}:${COGMENT_VERSE_MLFLOW_PORT}/tcp
    volumes:
      - ./data/mlflow:/data

  web_client:
    build: web_client
    environment:
      - COGMENT_VERSE_GRPCWEBPROXY_PUBLIC_URL
      - REACT_APP_GRPCWEBPROXY_URL=${COGMENT_VERSE_GRPCWEBPROXY_PUBLIC_URL}
      - PORT=${COGMENT_VERSE_WEBCLIENT_PORT}
    restart: on-failure
    ports:
      - ${COGMENT_VERSE_WEBCLIENT_EXPOSED_PORT}:${COGMENT_VERSE_WEBCLIENT_PORT}/tcp

  grpcwebproxy:
    build:
      context: ./grpcwebproxy
      dockerfile: ../grpcwebproxy.dockerfile
    restart: on-failure
    environment:
      - COGMENT_VERSE_ORCHESTRATOR_ENDPOINT
      - COGMENT_VERSE_GRPCWEBPROXY_PORT
    ports:
      - ${COGMENT_VERSE_GRPCWEBPROXY_EXPOSED_PORT}:${COGMENT_VERSE_GRPCWEBPROXY_PORT}/tcp
    depends_on:
      - orchestrator
