version: "3.7"

services:
  # Base images, they are defined here to be built when running `docker compose build`
  base_python:
    build: base_python
    image: cogment/cogment-verse-python-sdk:${IMAGE_TAG:-local}
    command: echo "Not a service, exiting"

  orchestrator:
    build:
      context: .
      dockerfile: orchestrator.dockerfile
    environment:
      - COGMENT_VERSE_PRETRIAL_HOOK_ENDPOINT
      - COGMENT_VERSE_TRIAL_DATASTORE_ENDPOINT
      - TRIAL_LIFECYCLE_PORT=${COGMENT_VERSE_ORCHESTRATOR_PORT}
      - TRIAL_ACTOR_PORT=${COGMENT_VERSE_ORCHESTRATOR_PORT}
    depends_on:
      - trial_datastore

  trial_datastore:
    image: cogment/trial-datastore:v0.1.2
    restart: on-failure
    environment:
      - COGMENT_TRIAL_DATASTORE_PORT=${COGMENT_VERSE_TRIAL_DATASTORE_PORT}
      # Maximum size (in bytes) of the trial datastore memory storage
      # Set to 1GB = 1073741824B
      - COGMENT_TRIAL_DATASTORE_MEMORY_STORAGE_MAX_SAMPLE_SIZE=1073741824
      # Log level (trace/debug/info/warn/error)
      - COGMENT_TRIAL_DATASTORE_LOG_LEVEL=info

  model_registry:
    image: cogment/model-registry:v0.2.0
    restart: on-failure
    environment:
      - COGMENT_MODEL_REGISTRY_PORT=${COGMENT_VERSE_MODEL_REGISTRY_PORT}
      - COGMENT_MODEL_REGISTRY_SENT_MODEL_VERSION_DATA_CHUNK_SIZE=2097152
    volumes:
      - ./data/model-registry:/data

  environment:
    image: cogment/cogment-verse-environment:${IMAGE_TAG:-local}
    build:
      context: ./environment
      args:
        BASE_PYTHON_IMAGE: cogment/cogment-verse-python-sdk:${IMAGE_TAG:-local}
    environment:
      - COGMENT_VERSE_ENVIRONMENT_PORT
      - COGMENT_VERSE_ENVIRONMENT_PROMETHEUS_PORT
    init: true # xvfb-run hang fix?
    tty: true

  torch_agents: &torch_agents
    build:
      context: torch_agents
      dockerfile: cpu.dockerfile
      args:
        BASE_PYTHON_IMAGE: cogment/cogment-verse-python-sdk:${IMAGE_TAG:-local}
    restart: on-failure
    environment:
      - PYTHONUNBUFFERED=1
      - COGMENT_VERSE_TORCH_AGENTS_PORT
      - COGMENT_VERSE_TORCH_AGENTS_PROMETHEUS_PORT
      - COGMENT_VERSE_MODEL_REGISTRY_ENDPOINT
      - COGMENT_VERSE_TRIAL_DATASTORE_ENDPOINT
      - COGMENT_VERSE_ENVIRONMENT_ENDPOINT
      - COGMENT_VERSE_ORCHESTRATOR_ENDPOINT
      - COGMENT_VERSE_ACTOR_ENDPOINTS
      - MLFLOW_TRACKING_URI=${COGMENT_VERSE_MLFLOW_TRACKING_URI}
      - MPLBACKEND=Agg
    depends_on:
      - orchestrator
      - environment
      - model_registry
      - trial_datastore
      - mlflow

  tf_agents: &tf_agents
    build:
      context: tf_agents
      args:
        BASE_PYTHON_IMAGE: cogment/cogment-verse-python-sdk:${IMAGE_TAG:-local}
    restart: on-failure
    environment:
      - PYTHONUNBUFFERED=1
      - COGMENT_VERSE_TF_AGENTS_PORT
      - COGMENT_VERSE_TF_AGENTS_PROMETHEUS_PORT
      - COGMENT_VERSE_MODEL_REGISTRY_ENDPOINT
      - COGMENT_VERSE_TRIAL_DATASTORE_ENDPOINT
      - COGMENT_VERSE_ENVIRONMENT_ENDPOINT
      - COGMENT_VERSE_ORCHESTRATOR_ENDPOINT
      - COGMENT_VERSE_ACTOR_ENDPOINTS
      - MLFLOW_TRACKING_URI=${COGMENT_VERSE_MLFLOW_TRACKING_URI}
      - MPLBACKEND=Agg
    depends_on:
      - orchestrator
      - environment
      - model_registry
      - trial_datastore
      - mlflow

  client:
    build: client
    command: --params_path=/cogment-verse/client/run_params.yaml
    environment:
      - COGMENT_VERSE_RUN_ENDPOINTS
    volumes:
      - ./run_params.yaml:/cogment-verse/client/run_params.yaml

  grafana:
    image: cogment/cogment-verse-grafana:${IMAGE_TAG:-local}
    build: ./grafana
    restart: on-failure
    user: 0:0
    environment:
      - COGMENT_VERSE_GRAFANA_PORT
      - COGMENT_VERSE_PROMETHEUS_URL
    ports:
      - ${COGMENT_VERSE_GRAFANA_EXPOSED_PORT}:${COGMENT_VERSE_GRAFANA_PORT}/tcp
    volumes:
      - ./data/grafana:/data
    depends_on:
      - prometheus

  prometheus:
    image: cogment/cogment-verse-prometheus:${IMAGE_TAG:-local}
    build: ./prometheus
    restart: on-failure
    user: 0:0
    environment:
      - COGMENT_VERSE_PROMETHEUS_PORT
      - COGMENT_VERSE_PROMETHEUS_TARGETS
    volumes:
      - ./data/prometheus:/prometheus

  mlflow:
    build: ./mlflow
    restart: on-failure
    environment:
      - COGMENT_VERSE_MLFLOW_PORT
    ports:
      - ${COGMENT_VERSE_MLFLOW_EXPOSED_PORT}:${COGMENT_VERSE_MLFLOW_PORT}/tcp
    volumes:
      - ./data/mlflow:/data

  web-client:
    build: web-client
    environment:
      - NODE_ENV=development
      - CHOKIDAR_USEPOLLING=true
      - REACT_APP_APP_VERSION=dev
      - REACT_APP_GRPCWEBPROXY_URL=${COGMENT_VERSE_GRPCWEBPROXY_PUBLIC_URL}
      - PORT=${COGMENT_VERSE_WEBCLIENT_PORT}
    restart: on-failure
    ports:
      - ${COGMENT_VERSE_WEBCLIENT_EXPOSED_PORT}:${COGMENT_VERSE_WEBCLIENT_PORT}/tcp
    depends_on:
      - grpcwebproxy

  grpcwebproxy:
    build:
      context: ./grpcwebproxy
      dockerfile: ../grpcwebproxy.dockerfile
    restart: on-failure
    environment:
      - COGMENT_VERSE_ORCHESTRATOR_ENDPOINT
      - COGMENT_VERSE_GRPCWEBPROXY_PORT
    ports:
      - ${COGMENT_VERSE_GRPCWEBPROXY_EXPOSED_PORT}:${COGMENT_VERSE_GRPCWEBPROXY_PORT}/tcp
    depends_on:
      - orchestrator
